\documentclass[a4paper,12pt]{article}
\usepackage{stdtemplate}

\title{\vspace{-50pt}\bfseries{\Large{CSC373 Algorithms}}\vspace{-30pt}}
\author{\normalfont{Jonah Chen}}
\date{}
\begin{document}
\maketitle
\tableofcontents
\sffamily
\section{Divide and Conquer}
\begin{itemize}
    \item Divide and Conquer algorithm:
    \begin{enumerate}
        \item Divide problem of size $n$ into $a$ smaller subproblems of size $n/b$ each
        \item Recursively solve each subproblem
        \item Combine the subproblem solutions into the solution of the original problem
    \end{enumerate}
    \item Runtime: $T(1)=c$ and $T(n)=a T(n/b) + cn^d$ for $n>1$
    \item Master Theorem: $T(n)$ depends on relation between $a$ and $b^d$. \begin{equation}
        \begin{cases}
            a<b^d: T(n)=\Theta(n^d)\\
            a=b^d: T(n)=\Theta(n^d\log n)\\
            a>b^d: T(n)=\Theta(n^{\log_b a})
        \end{cases}
    \end{equation}
    \begin{itemize}
        \item Note that the running time does not depend on the constant $c$
        \item In many algorithms $d=1$ (combining take linear time)
    \end{itemize}
    \item Examples:
    \begin{itemize}
        \item Merge sort --- sorting array of size $n$ ($a=2,\,b=2,\,d=1\rightarrow a=b^d$) so $T(n)=\Theta(n\log n)$
        \item Binary search --- searching sorted array of size $n$ ($a=1,\,b=2,\,d=0\rightarrow a=b^d$) so $T(n)=\Theta(\log n)$
    \end{itemize}
\end{itemize}
\subsection{Karatsuba Multiplication}
\begin{itemize}
    \item \bluebf{Add} two binary $n$-bit numbers naively takes $\Theta(n)$ time
    \item \bluebf{Multiply} two binary $n$-bit numbers naively takes $\Theta(n^2)$ time
    \item Divide and Conquer approaches \begin{enumerate}
        \item Multiply $x$ and $y$. We can divide them into two parts\begin{align}
            x &= x_1\cdot 2^{n/2} + x_0\\
            y &= y_1\cdot 2^{n/2} + y_0\\
            x\cdot y &= x_1\cdot y_1\cdot 2^n + (x_1\cdot y_0 + x_0\cdot y_1)\cdot 2^{n/2} + x_0\cdot y_0 \label{eq:mul}
        \end{align}
        \begin{itemize}
            \item $T(n)=4T(n/2)+cn; T(1)=c$
            \item $a=4,b=2,d=1$ Master Theorem case 3, $T(n)=\Theta(n^{\log_2 4})=\Theta(n^2)$.
            \item This is the same complexity of the naive approach, making this approach useless.
        \end{itemize}
        
        \item Reconsider \eqref{eq:mul}, we may rewrite $(x_1\cdot y_0+x_0\cdot y_1)$ as $(x_1+x_0)\cdot(y_1+y_0)-x_1\cdot y_1-x_0\cdot y_0$
        \begin{equation}
            x\cdot y = x_1\cdot y_1\cdot 2^n + ((x_1+x_0)\cdot(y_1+y_0)-x_1\cdot y_1-x_0\cdot y_0)\cdot 2^{n/2} + x_0\cdot y_0
        \end{equation}
         
        \begin{itemize}
            \item $T(n)=3T(n/2)+cn; T(1)=c$
            \item $a=3,b=2,d=1$, Master Theorem case 3, $T(n)=\Theta(n^{\log_2 3})=\Theta(n^{\log_2 3})\approx\Theta(n^{1.585})$
            \item Minor issue: a carry may increase $x_1+x_0$ and $y_1+y_0$ to $\frac{n}{2} + 1$. We can easily prove this by isolating the carry bit and reevaluating the complexity.
        \end{itemize}
    \end{enumerate}
    \item To deal with integers which doesn't have a power of 2 number of bits, we can pad the numbers with 0s to make them have a power of 2 number of bits. This may at most increase the complexity by 3x.
    \item 1971: $\Theta(n\cdot\log n\cdot \log\log n)$
    \item 2019: Harvey and van der Hoeven $\Theta(n\log n)$. We do not know if this is optimal.
\end{itemize}
\subsection{Strassen's MatMul Algorithm}
\begin{itemize}
    \item Let $A$ and $B$ be two $n\times n$ matrices (for simplicity $n$ is a power of 2), we want to compute $C=AB$.
    \item The naive approach takes $\Theta(n^3)$ time.
    \begin{enumerate}
        \item Divide $A$ and $B$ into $4$ submatrices of size $\frac{n}{2}\times\frac{n}{2}$ each
        \begin{equation}
            A = \begin{bmatrix}
                A_{1} & A_{2}\\
                A_{3} & A_{4}
            \end{bmatrix}.
        \end{equation}
        Then, $C$ can be calculated with
        \begin{align}
            C_1 &= A_1B_1 + A_2B_3\\
            C_2 &= A_1B_2 + A_2B_4\\
            C_3 &= A_3B_1 + A_4B_3\\
            C_4 &= A_3B_2 + A_4B_4
        \end{align}
        \begin{itemize}
            \item $T(n)=8T(n/2)+cn^2; T(1)=c$
            \item $a=8,b=2,d=2$, case 3 $T(n)=\Theta(n^{\log_2 8})=\Theta(n^3)$
        \end{itemize}
        \item \textbf{Idea:} Compute $C_1,C_2,C_3,C_4$ with only $7$ multiplications, not 8.
        \begin{align}
            M_1 &= (A_2-A_4)(B_3+B_4)\\
            M_2 &= (A_1+A_4)(B_1+B_4)\\
            M_3 &= (A_1-A_3)(B_1+B_2)\\
            M_4 &= (A_1+A_2)B_4\\
            M_5 &= A_1(B_2-B_4)\\
            M_6 &= A_4(B_3-B_1)\\
            M_7 &= (A_3+A_4)B_1
        \end{align}
        With these we can compute $C_1,C_2,C_3,C_4$ with only additions of the $M$ matrices.
        \begin{align}
            C_1 &= M_1+M_2-M_4+M_6\\
            C_2 &= M_4+M_5\\
            C_3 &= M_6+M_7\\
            C_4 &= M_2-M_3+M_5+M_7
        \end{align}
        \begin{itemize}
            \item $T(n)=7T(n/2)+cn^2; T(1)=c$
            \item $a=7,b=2,d=2$, case 3 $T(n)=\Theta(n^{\log_2 7})=\Theta(n^{\log_2 7})\approx\Theta(n^{2.807})$
        \end{itemize}
    \end{enumerate}
    \item If $n$ is not a power of 2, we zero-pad the matrices to have $n$ as a power of two. This may increase the complexity by at most a factor of 7.
\end{itemize}

\subsection{Median of Unsorted Arrays}
\begin{itemize}
    \item For an unsorted array $A$, we can find the average, max, min, sum, etc. in linear time.
    \item The trivial algorithm is to sort $A$ then get the median. That takes $O(n\log n)$ time.
    \item We will solve a more general problem: Find the $k^{th}$ smallest element in $A$. (e.g. $A=5,2,6,7,4$, $\mathrm{Select}(A,1)=2, \mathrm{Select}(A,4)=6$)
    \item if $|A|=1$, then return $A[1]$. Otherwise find a splitter $s$ in arbitrary element of $A$. Partition $A$ into $A^+$ and $A^-$, then divide 
    \item $T(n)=T(\max(|A^-|,|A^+|))+cn=T(\max(i-1,n-i))+cn$.
    \item Worst case: $T(n)=T(n-1)+cn=\Theta(n^2)$
    \item Best case: $T(n)=T(n/2)+cn=\Theta(n)$. Suppose $b>1$, by the Master Theorem $T(n)=T(n/b)+cn=\Theta(n)$.
\end{itemize}

We define $s$ is a good splitter if $s$ is greater than $1/4$ of the elements of $A$ and less than $1/4$ of the elements of $A$. We can make the following observation:
\begin{enumerate}
    \item With this splitter, we will reduce the size to at most $3n/4$.
    \item Half the elements are good splitters.
\end{enumerate}

We should select splitter $s$ uniformly at random.
\begin{itemize}
    \item $P(\text{splitter is good})=\frac{1}{2}$
    \item $P(\text{splitter is bad})=\frac{1}{2}$
    \item We can show that the expected number of trials (splitter selections) until obtaining a good splitter is 2.
\end{itemize}
\subsubsection{Expected Runtime}
\begin{equation}
    \underbrace{n_0\to n_1\to n_2}_{\text{Phase 0, size}\leq n}\to \underbrace{n_3\to n_4}_{\text{Phase 1, size}\leq \frac{3}{4}n}\to \underbrace{n_5\to n_6}_{\text{Phase 2, size}\leq \frac{3}{4}^2 n}\to\dots
\end{equation}
\begin{itemize}
    \item Phase $j$: input size $\leq(\frac{3}{4})^j\,n$
    \item Random variable $y_j = $\# of recursive calls in phase $j$. Note that $E[y_j]=2$.
    \item Random variable $x_j = $\# of steps to all the recursive calls in phase $j$.
    \item Total number of steps is $x=x_0+x_1+x_2+\dots$.
    \item We can compute $E[x]=E[x_0]+E[x_1]+E[x_2]+\dots$.
    \begin{align}
        x_j&\leq c y_j \frac{3}{4}^j n\\
        E[x_j]&\leq c E[y_j] \frac{3}{4}^j n\leq 2c\frac{3}{4}^j n\\
        E[x]&=\sum_j E[x_j]\leq \sum_{j=1}^\infty 2c\frac{3}{4}^j n=\frac{2c}{1-\frac{3}{4}}n=8cn=\Theta(n)
    \end{align}
\end{itemize}

\subsubsection{Deterministic Algorithm}
\begin{itemize}
    \item If $|A|\leq 5$ then we sort $A$ and return the $k^{th}$ smallest.
    \item Otherwise, partition $A$ into $n/5$ groups of size $5$ each, then find the median of each group (constant time) and store in list $M$. This takes linear time.
    \item Select the median of $M$ with the Select algorithm, this is a good splitter.
    \item the worst case running time is $T(n)=T(\lceil\frac{n}{5}\rceil)+T(\lfloor{\frac{3n}{4}}\rfloor)+cn$.
    \item This recursive relation cannot be solved by the Master Theorem. We can prove using induction that $T(n)<20cn$.
\end{itemize}
\redbf{Question: Why groups of 5?}
\begin{itemize}
    \item With groups of 5, the total size of subproblems: $\frac{n}{5}+\frac{3n}{4}=\frac{19n}{20}<n$
    \item With groups of 3, the total size of subproblems: $\frac{n}{3}+\frac{3n}{4}=\frac{13n}{12}>n$, not sufficient.
    \item So group size of $5,7,9,11,\dots$ would also work.
\end{itemize}

\section{Closest Pair of Points}
\begin{itemize}
    \item Problem: Given a set of $n$ points, find the pair of points that are the closest in $O(n\log n)$.
\end{itemize}
\subsection{Closest Pair in 2D}
\begin{itemize}
    \item Divide: points roughly in half by drawing vertical line on midpoint
    \item Conquer: Find closest pair on each half, recursively.
    \item Combine: Find the closest pair $(p,q)$, $p\in L$, $q\in R$. However, there may be $\Theta(n^2)$ pairs.
\end{itemize}
\begin{itemize}
    \item Claim: Let $p=(x_p,y_p)\in B_L,q=(x_q,y_q)\in B_R$ with $y_p\leq y_q$. If $d(p,q)<\delta$ then there are at most \bluebf{six} other points (x,y) in $B$ such that $y_p\leq y\leq y_q$.
    \item Proof:
    \item $S_L=\{p'=(x,y):p'\neq p\in B_L\land y_p\leq y\leq y_q\}$ (other points on the left of the middle)
    \item $S_R=\{p'=(x,y):p'\neq q\in B_R\land y_p\leq y\leq y_q\}$ (other points on the right of the middle)
    \item Assume by contradiction that $|S_L\cup S_R|\geq 7$. WLOG assume $|S_L|\geq 4$.
    \item In a $\delta\times\delta$ square there are at least $4+1=5$ points. Divide the square into 4 smaller squares, by Pigeonhole Principle, there is a square with at least 2 points, whose distance is at most $\delta/\sqrt2$. This contradicts the assumption that the closest pair on the left is at most $\delta$.
\end{itemize}
\begin{itemize}
    \item Then, we can sort everything in the $y$ axis, and check the next seven points by the $y$ coordinate for the minimum distance. This takes linear time.
    \item We only need to modify the combine step in the algorithm so it's $\Theta(n)$ runtime.
\end{itemize}

\begin{algorithm}
\caption{Closest Pair in 2D}
\begin{algorithmic}[1]
\Procedure{ClosestPair}{$P$}
\State $P_x:=\text{the list of points in }P\text{ sorted by x-coordinate}$
\State $P_y:=\text{the list of points in }P\text{ sorted by y-coordinate}$
\EndProcedure
\Procedure{RCP}{$P_x,P_y$}
\If{$|P_x|\leq 3$}
\Return{$\text{brute force}(P_x)$}
\EndIf
\State $L_x:=\text{the first half of }P_x;R_x:=\text{the second half of }P_x$
\State $m:=(\text{max x-coordinate of }L_x+\text{min x-coordinate of }R_x)/2$
\State $L_y:=\text{sublist of }P_y\text{ with points in }L_x$
\State $R_y:=\text{sublist of }P_y\text{ with points in }R_x$
\State $(p_L,q_L):=\text{RCP}(L_x,L_y);(p_R,q_R):=\text{RCP}(R_x,R_y)$
\State $\delta:=\min\{d(p_L,q_L),d(p_R,q_R)\}$
\If {$\delta=d(p_L,q_L)$}
\State $p:=p_L;q:=q_L$
\Else
\State $p:=p_R;q:=q_R$
\EndIf
\State $B:=\text{sublist of }P_y\text{ with points in }[m-\delta,m+\delta]$
\Foreach {$p$ in $B$}
\Foreach {next seven $q$ after $p$ in $B$}
\If {$d(p,q)<d(p^*,q^*)$} $(p^*,q^*):=(p,q)$
\EndIf
\EndForeach
\EndForeach
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{itemize}
    \item So $T(n)=2T(\frac{n}{2})+cn$, which is $O(n\log n)$.
\end{itemize}

\section{Greedy Algorithms}
\begin{itemize}
    \item There is an optimization problem: given an input, find a solution that minimize/maximize an objective function $f$ under some constraint.
    \item Build the solution incrementally in stages
    \item At each stage, extend the solution greedily and irrevocably.
    \item For some problems this gives optimal solutions (i.e. MST), but for other problems it does not.
    \item The order of the stages is very important.
\end{itemize}

\subsection{Interval Scheduling}
\begin{itemize}
    \item Input: $n$ intervals, with interval $j$ starts at time $s_j$ and finishes at time $f_j$.
    \item Output: maximum-size set of intervals that do not overlap
    \item Naive algorithm is to try each subset of $n$ intervals by brute force $O(2^n)$, way too slow.
    \item Greedy algorithm sorts interval in some order, then if it doesn't overlap then add it to the solution.
    \item \redbf{What order gives the biggest feasible set?}
    \begin{enumerate}
        \item Increasing start time: $s_j$
        \item Increasing finish time: $e_j$
        \item Shortest interval: $e_j-s_j$
        \item Fewest conflicts
    \end{enumerate}
    \item The question is which one is optimal? Option 1 has a clear counterexample with one very long interval that overlaps each other interval. Option 3 is also not optimal using a short interval between two long interval. Option 4 is not optimal (not very clear counterexample).
    \item Option 2 is optimal. The intuition is that chosing these intervals first will leave the most time for the rest of the intervals.
    \item To find if the interval is compatible, we just need to check if the start time of the new interval is greater than the finish time of the latest scheduled interval.
\end{itemize}
\subsubsection{Proof of optimality}
\begin{itemize}
    \item Suppose for contradiction that this greedy algorithm is not optimal
    \item Say greedy selects interval $i_1,\dots,i_k$ sorted by increasing finish time.
    \item Suppose the optimal schedule $j_1,\dots,j_m$ has $m>k$ intervals, and sort by increasing finish time. Consider an optimal schedule that can match the greedy $j_1=i_1,\dots,j_r=i_r$ for the greatest possible $r$.
    \item By the nature of the greedy algorithm, then $i_{r+1}$ finishes the earliest amongst the compatible intervals remaining. So consider the schedule $S:i_1,\dots,i_r,\mathbf{i_{r+1}},j_{r+2},\dots,j_m$. 
    \item This is also optimal (contains $m$ intervals) but matches the greedy algorithm by one more position. This is a contradiction.
\end{itemize}

\subsection{Interval Partitioning}
\begin{itemize}
    \item Motivation: given a set of lecture time intervals, schedule them into as few classrooms as possible.
    \item Input: $n$ intervals, interval $j$ starts at $s_j$ and finishes at $f_j$.
    \item Output: group interval into fewest partitions such that intervals in each partition are compatible.
    \item An idea is to find the maximum set of compatible intervals using the previous algorithm. This doesn't work.
    \item We can try the same orders as the previous problem, but only earliest start time is optimal in this case.
    \item To implement it efficiently, we will use a heap with increase-key operation.
\end{itemize}
\subsubsection{Proof of optimality}
\begin{itemize}
    \item We define the \bluebf{depth} at time $t$ as the number of intervals that contain time $t$. The \bluebf{maximum depth} $d_{max}$ is the maximum depth over all times.
    \item Clearly, the number of partitions needed is at least $d_{max}$. We will show that this greedy algorithm create only $d_{max}$ partitions.
    \item Let $d$ be the number of partition the greedy algorithm opened.
    \item Partition $d$ was created because there was an interval $j$ that overlaps with some previously scheduled interval in each of the $d-1$ other partitions.
    \item This means that for $d-1$ intervals, their start times are all before $s_j$ and their finish times are all after $f_j$ (otherwise $j$ must be compatible). Hence, the depth at $s_j$ is exactly $d$.
    \item Thus, $d_{max}\geq d$ so the greedy algorithm is optimal.
    \item Warning: this proof rely on the fact that the start time
\end{itemize}

\subsection{Minimum Lateness Scheduling}
\begin{itemize}
    \item $n$ intervals: $1,2,\dots,n$, with each interval $j$ requires $t_j$ units of time and has deadline $d_j$.
    \item The lateness of an interval is $l_j=\max\{0,f_j-d_j\}$
    \item Output the schedule that minimizes the maximum lateness $L=\max_jl_j$.
    \item Fact 1: there is an optimal schedule with no gaps.
    \item The naive algorithm is to try all possible schedules, which is $O(n!)$.
    \item For greedy algorithm, we will sort intervals in this order
    \begin{enumerate}
        \item Shortest processing time, $t_j$
        \item Smallest slack first $d_j-t_j$
        \item Earliest deadline first $d_j$
    \end{enumerate}
    \item 1 and 2 are not optimal, and they have simple counterexamples with $n=2$.
    \item an \bluebf{inversion} is two intervals $i,j$ such that $d_i>d_j$ but $i$ is scheduled before $j$.
    \item \redbf{Note that in general:} Define inversion as a violation of what your order is, then prove that inversion is slightly worse or bad.
\end{itemize}
\subsubsection{Proof of optimality}
\begin{itemize}
    \item Observation 1: The greedy algorithm has no gaps.
    \item Observation 2: The greedy algorithm has no inversions.
    \item We will prove
    \item \bluebf{Claim 1}: If a schedule $S$ with no gaps has an inversion, then $S$ has a pair of inverted intervals that are adjacent.
    \item Suppose for contradiction that there is $S$ with an inversion but does not have adjacent intervals that are inverted. Then, $d_j<d_i\leq d_{i+1}\leq d_{i+2}\leq\dots\leq d_{j-1}\leq d_j$, so $d_j<d_j$ which is a contradiction.
    \item \bluebf{Claim 2}: All schedules with no gaps and no inversions have the same lateness
    \item Let $S$ and $S'$ be two distinct schedules with no gaps and no inversions
    \item Note $S$ and $S'$ differ only by the schedule of intervals with the same deadline
    \item Consider the intervals with the same deadline, they must be adjacent in both $S$ and $S'$. As a group, the maximum lateness of these intervals is the same because the group will finish at the same time.
    \item \bluebf{Claim 3}: Swapping adjacent inverted interval does not increase lateness and reduces the total number of inversions by one.
    \item Let $i$ and $j$ denote two adjacent inverted intervals in the schedule $S$. By swapping $i$ and $j$, we get a schedule $S'$ with one fewer inversion.
    \item Let $l$ and $l'$ be the lateness before/after swap. $\forall k\neq i,j, l_k=l'_k$.
    \item We know $d_i>d_j$ because it was an inversion. Thus, $l_j\geq l_j'$ and $l_j\geq l_i'$
    \item $l_j=f_j-d_j\geq f_i'-d_i=l_i'$
    \item Suppose by contradiction that the greedy schedule $S$ is not optimal.
    \item Let $S^*$ is the schedule with no gaps and fewest inversion.
    \item \textbf{Case 1:} $S^*$ has no inversion. By claim 2, this is contradiction.
    \item \textbf{Case 2:} $S^*$ has at least one inversion. By claim 1, it must have two inverted intervals that are adjacent. By claim 3, we can swap them to get a schedule with no greater lateness with one fewer inversion so it must be optimal. Then $S^*$ does not have the fewest inversions among all optimal schedules. This is also a contradiction.
\end{itemize}
\subsection{Huffman Code}
\begin{itemize}
    \item Given an alphabet $\Gamma$: a set of $n$ symbols
    \item You need to encode symbols using binary code. 
    \item Fixed length code requires $\lceil\log n\rceil$ bits per symbol.
    \item This is easy to decode, but this is not optimal
    \begin{itemize}
        \item ``e'': 12.7\% of letters in english
        \item ``z'': 0.07\% of letters in english
        \item We should give a shorter code to ``e'' than ``z''.
    \end{itemize}
    \item The goal is to find a code that minimize the length of text coding
    \item Variable length code can save space, but is harder to decode. Suppose $\Gamma=\{a,b,c\}$ and $a=1,b=01,c=010$. Then $0101$ is ambiguous as $bb$ or $ca$.
    \item A \bluebf{prefix code} is a code where no codeword is a prefix of any other. Scan from left to right until you see a codeword.
    \item Prefix code can be represented as a binary tree (edges are leaf), with the leaves as the symbols.
    \item The efficiently of a code is the weighted height of this tree, where the weight of a leaf is the probability of that symbol.
\end{itemize}
\subsection{Prefix Code Problem}
\begin{itemize}
    \item Input: A set of $\Gamma$ with their frequencies $f:\Gamma\to\R$ where $\sum_{x\in\Gamma}f(x)=1$
    \item Output: Binary tree $T$ representing the optimal prefix code for $\Gamma,f$
    \item Optimal Solution is a tree with the weighted average height $AD(T)$ of the tree is minimum.
    \item \textbf{Fact 1:} An optimal tree is a full binary tree, where each internal has two children.
    \item \textbf{Fact 2:} In an optimal tree $T$, $\forall x,y;f(x)<f(y)\implies \mathrm{depth}_T(x)\geq\mathrm{depth}_T(y)$.
    \item \textbf{Fact 3:} If $x,y$ have minimum frequency, then there is an optimal tree $T$ such that $x,y$ are siblings and are at max depth.
    \item \textbf{Huffman's Algorithm}: Combine the two smallest frequencies into a new symbol $z$ with frequency $f(z)=f(x)+f(y)$, and repeat recursively until there are only two symbol left, in which case we assign them $0$ and $1$.
\end{itemize}
\subsubsection{Proof of Optimality}
\begin{itemize}
    \item We will prove by induction. The base case for $n=2$ is trivial.
    \item Induction hypothesis: for all $\Gamma,f$ for $n-1$ symbols, the algorithm produces an optimal tree.
    \item Let $\Gamma,f$ be an alphabet with frequencies with $n$ symbols. Let $H$ be the tree produced by the algorithm.
    \item  The algorithm constructed by $H$ by: (1) replacing two symbols $x,y\in\Gamma$ of minimum frequency with a new symbol $z$, and
    \begin{align}
        \Gamma'&=(\Gamma\setminus\{x,y\})\cup\{z\}\\
        f'(\alpha)&=\begin{cases}
            f(x)+f(y),&\alpha=z\\
            f(\alpha),&\text{otherwise}
        \end{cases}
    \end{align}
    \item We know that $x,y$ are siblings (with parent $z$) in $H$
    \item Since $H'$ has $n-1$ symbols, $H'$ is optimal for $\Gamma',f'$
    \item We know $AD(H)=AD(H')+f(x)+f(y)$
    \item Now take an optimal tree $T$ for $(\Gamma,f)$ where $x,y$ are siblings. We know $T$ exists by fact 3.
    \item Let $T'$ be the tree constructed from $T$ by removing $x,y$ and replacing their parent with symbol $z$ (and $f(z)=f(x)+f(y)$)
    \item Clearly, $T'$ is a prefix code for $(\Gamma',f')$ with $AD(T)=AD(T')+f(x)+f(y)$
    \item We know $AD(T')\geq AD(H')$ as $H'$ is optimal for $(\Gamma',f')$ Thus, $AD(T)-f(x)-f(y)=AD(T')\geq AD(H')=AD(H)-f(x)-f(y)\implies AD(T)\geq AD(H)$. but $T$ is optimal so $H$ is optimal.
\end{itemize}
\subsection{Dijkstra's Single-Source Shortest Path Algorithm}
\begin{itemize}
    \item Input: A weighted directed graph $G=(V,E)$ with weights $w:E\to\R_+$ and a source vertex $s\in V$
    \item Output: Length of the shortest path from $s$ to every other node $t\in V$
    \item Suppose $R$ is a subset of nodes of $G$ that includes $s.$ A path $s\to v$ is a \bluebf{$R$-path} from $s\to v$ is restricted to contain only in $R$ before reaching $v.$
    \item At a high level, the algorithm maintains a set $R\subseteq V,$ (at the beginning $s$, and at the end $V$)
    \item The algorithm also contains $d(v)$ where
    \begin{enumerate}
        \item if $v\in R$, then $d(v)$ is the length of the shortest path from $s\to v.$ This is exactly what we want.
        \item if $v\in V\setminus R$, then $d(v)$ is the length of the shortest $s\to v$ $R$-path.
    \end{enumerate}
    \item The goal of the algorithm is to have $R$ contain all the nodes. In each iteration, the algorithm adds one more node from $R$ and maintains its properties.
    \item The algorithm will greedily choose the node $u\in V\setminus R$ with the minimum $d(u)$.
    \item We know that $d(u)$ is the length of the shortest $s\to u$ $R$-path. We will show that this is also the length of the shortest $s\to u$ path. If the claim is true, we can
    \begin{enumerate}[label=\alph*)]
        \item move $u$ to $R$ and still maintain property 1.
        \item update $d(v)$ for nodes in $V\setminus R$ to satisfy property 2.
    \end{enumerate}
    \item The proof of the claim is simple, suppose by contradiction that there is a shorter path $P$ with $len(P)<d(u).$ $P$ must cross from $R$ to $V\setminus R$ at least once. Let $v$ be the first node this $P$ reaches in $V\setminus R,$ so first section of $P,$ we denote $P_1$ from $s\to v$ is an $R$-path. As all the weights are non-negative, $len(P)\geq len(P_1)\geq d(v)\geq d(u),$ so $len(P)$ cannot be less than $d(u),$ which is a contradiction.
    \item Upon adding $u$ to $R,$ we gain more freedom for $R$-paths. For points in $V\setminus R,$ we can choose to retain the previous $R$-path or to use $u$ for the new $R$-path. Thus, for some $v\in V\setminus R,$
    \begin{itemize}
        \item if the shortest $R$-path does not contain $u,$ do nothing.
        \item if the shortest $R$-path contains $u,$ then the new $d(v)=d(u)+w(u,v)$. \begin{itemize}
            \item Note that $u$ must be the last node in the shortest $R$-path. Suppose there is another node $r\in R$ where $s\to u\to r\to v$ is shorter than $s\to u\to v.$
            \item Since $r\in R$ when $u$ was still in $V\setminus R,$ $d(r)$ must be the length of the shortest path of $R\setminus\{u\}.$ So in this case, the shortest path to $v:s\to r\to v$ doesn't require $u.$
        \end{itemize}
    \end{itemize}
    \item So for every $v\in V\setminus R,$ we update $d(v)\gets\min\{d(v),d(u)+w(u,v)\}.$
    \item \begin{algorithmic}[1]
        \Procedure{Dijkstra}{}
        \State $R\gets\{s\}$
        \State $d(s)\gets 0$
        for each nodes $v\neq s$ do if $(s,v)$ is an edge then $d(v)\gets w(s,v), p(v)\gets s$ else $d(v)\gets\infty,p(v)\gets Nil$
        \While{$R\neq V$}
            \State $u\gets$ node not in $R$ with the minimum $d(u)$
            \State $R\gets R\cup\{u\}$
            \Foreach{$v$ s.t. $(u,v)$ is an edge}
                \If{$d(v)>d(u)+w(u,v)$}
                    \State $d(v)\gets d(u)+w(u,v)$
                    \State $p(v)\gets u$
                \EndIf
            \EndForeach
        \EndWhile
        \EndProcedure
        \end{algorithmic}
    \item \redbf{Note} that the algorithm does not work with negative weights.
\end{itemize}
\section{Dynamic Programming}
\begin{itemize}
    \item DP works on problems that have the \bluebf{optimal substructure property,} i.e. optimal solution can be computed efficiently from the optimal solution of subproblems.
    \item The method is to break the problem down into subproblems, solve each subproblem only once and \redbf{store} the solutions.
    \item When solving a subproblem, we look up previously computed solutions instead of recomputing it.
    \item Storing the solution to subproblems is called \bluebf{memoization.}
    \item DP is \textbf{different} from divide and conquer because:
    \begin{itemize}
        \item Divide and conquer is a special case of dynamic programming where the subproblems that are solve \redbf{never overlap}, so there's no need for memoization to avoid recomputing some previously solved solution.
    \end{itemize}
\end{itemize}
\subsection{DAG Shortest Path (Toy Problem)}
\begin{itemize}
    \item Given some directed acyclic graph, find the shortest path from the root to any leaf.
    \begin{enumerate}
        \item The greedy algorithm is not optimal (very simple counterexample).
        \item Naive recursion works, but it is very slow.
        \begin{itemize}
            \item If each node has $k$ children, then $T(0)=c,T(n)=kT(n-1)+c$ then the runtime is $\Theta(k^n)$.
            \item Note that in naive recursion, we are solving the same subproblems over and over again. Dynamic programming will aim to solve this problem.
        \end{itemize}
        \item Dynamic programming. We can solve the problem by caching the solutions of the subproblems and in a bottom-up order rather than top-down. With \begin{align}
            \mathrm{COST}(i)\gets\min\{&\mathrm{COST}(\mathrm{nextup}(i))+\mathrm{costup(i)},\\&\mathrm{COST}(\mathrm{nextdown}(i))+\mathrm{costdown(i)}\}.
        \end{align}
        The cost of this approach is equal to the number of nodes, which is $\Theta(n^k).$
    \end{enumerate}
\end{itemize}
\subsection{Weighted Interval Scheduling}
\begin{itemize}
    \item In normal interval scheduling, every interval is of equal importance. So we want to schedule the maximum number of intervals.
    \item Now, each interval have a weight (as some jobs are more important than others), so we want to schedule a set of compatible intervals with the maximum total weight.
    \item Clearly, the greedy algorithm by earliest finish time is not optimal. Sorting by weight is not optimal either. There are obvious counterexamples. Actually, no greedy approaches will work.
\end{itemize}

\subsubsection{Dynamic Programming Approach}
\begin{itemize}
    \item We will sort the intervals by increasing finish time $f_1\leq f_2\leq\cdots\leq f_n.$
    \item The \textbf{predecessor} of interval $p[j]$ is the largest index $i$ such that $f_i\leq s_j.$ ($p[j]=0$ if interval $j$ has no predecessors) We can find $p[j]$ in $O(\log j)$ time by doing binary search.
    \item Let $S$ be the optimal subset of intervals (for intervals $\{1,\dots,n\}$)
    \item Consider the last interval $n,$ either
    \begin{enumerate}
        \item $n\notin S,$ then $S$ is optimal for intervals $\{1,\dots,n-1\}.$
        \item $n\in S,$ then $S=\{n\}\cup$ optimal subsets of intervals $\{1,\dots,p[n]\}.$
    \end{enumerate}
    \item $S$ is the best of case $1$ and case $2,$ (that with the highest total weight).
    \item For dynamic programming in general, we want to define our notation and subproblems \eqref{weighted_interval_subproblem} to reach the \bluebf{Bellman equation} \eqref{weighted_interval_bellman}.
    \begin{align}
        OPT(j)&=\text{ max weight for intervals in }\{1,\dots,j\}\label{weighted_interval_subproblem}\\
        OPT(j)&=\begin{cases}
            0 & \text{if }j=0\\
            \max\{\mathrm{OPT}(j-1),\mathrm{OPT}(p[j])+w[j]\} & \text{if }j>0
        \end{cases}\label{weighted_interval_bellman}
    \end{align}
    \item We can use a top-down approach with recursion
    \begin{algorithmic}[1]
        \Procedure{TopDown}{$w,s,f$}
        \State Sort the intervals $(w,s,f)$ by increasing finish time $f_1\leq f_2\leq\cdots\leq f_n.$
        \State Compute $p[j]$ for $j=1,\dots,n.$ using binary search for each $p[j].$
        \State Let $OPT$ be a global array of size $n+1$ where $OPT[0]=0$
        \State\Return TD-OPT($n$)
        \EndProcedure
        \Procedure{TD-OPT}{$j$}
        \If{$OPT[j]$ is not initialized}
            \State TD-OPT$[j]\gets\max\{\text{TD-OPT}(j-1),\text{TD-OPT}(p[j])+w[j]\}$
        \EndIf
        \State\Return $OPT[j]$
        \EndProcedure
    \end{algorithmic}
    \item However, a bottom up approach is preferred as it is more simpler and shows that you understand dynamic programming better.
    \begin{algorithmic}[1]
        \Procedure{BottomUp}{$w,s,f$}
        \State Sort the intervals $(w,s,f)$ by increasing finish time $f_1\leq f_2\leq\cdots\leq f_n.$
        \State Compute $p[j]$ for $j=1,\dots,n.$ using binary search for each $p[j].$
        \State Let $\mathrm{OPT}$ be an array of size $n+1$ where $\mathrm{OPT}[0]=0$
        \For{$j=1$ to $n$}
            \State $\mathrm{OPT}[j]\gets\max\{\mathrm{OPT}[j-1],\mathrm{OPT}[p[j]]+w[j]\}$
        \EndFor
        \State \Return $OPT[n]$
        \EndProcedure
    \end{algorithmic}
    \item The runtime of both approaches is $\Theta(n\log n)$ due to the sorting and binary search. The space complexity is $\Theta(n)$.
    \item \redbf{Warning}: We need to be careful with our implementation. If we do not store enough information, we will still be solve the same problem multiple times. For example, the following approach does not work. \begin{algorithmic}[1]
        \Procedure{NaiveRec}{$j$}
        \If{$j=0$}
            \State \Return 0
        \Else
            \State \Return $\max\{\mathrm{NaiveRec}(j-1),\mathrm{NaiveRec}(p[j])\}$
        \EndIf
        \EndProcedure
    \end{algorithmic}
    \item Notes on bottom up versus top down:
    \begin{itemize}
        \item Top down may be preferred when not all sub-solutions need to be computed on some inputs, and one does not need to think about the right order as much.
        \item Bottom up may be preferred when all sub-solutions will anyway need to be computed, and it is sometimes faster because it prevents the overhead of recursive calls.
    \end{itemize}
    \item One way of figuring out what subproblems to define is to think ``we already know what the optimal solution is, then what is needed to reach that.''
    \item We can modify the algorithm to return the actual set of intervals that are chosen. There are two ways of doing this.
    \begin{enumerate}
        \item Computing the optimal set \textbf{simultaneously} with the optimal weight.
        \begin{algorithmic}[1]
            \Procedure{BottomUp}{$w,s,f$}
            \State Sort the intervals $(w,s,f)$ by increasing finish time $f_1\leq f_2\leq\cdots\leq f_n.$
            \State Compute $p[j]$ for $j=1,\dots,n.$ using binary search for each $p[j].$
            \State Let $\mathrm{OPT}$ be an array of size $n+1$ where $\mathrm{OPT}[0]=0$
            \State Let $S$ be an array of size $n$ where $S[0]\gets\emptyset.$
            \For{$j=1$ to $n$}
                \State $\mathrm{OPT}[j]\gets\max\{\mathrm{OPT}[j-1],\mathrm{OPT}[p[j]]+w[j]\}$
                \If{$\mathrm{OPT}[j]>OPT[j-1]$}
                    \State $S[j]\gets S[p[j]]\cup\{j\}$
                \Else
                    \State $S[j]\gets S[j-1]$
                \EndIf

            \EndFor
            \State \Return $S[n]$
            \EndProcedure
        \end{algorithmic}
        \item Computing the optimal set \textbf{after} the optimal weight is computed.
        \begin{algorithmic}[1]
            \Procedure{GetSet}{$OPT,p$}
            \State Let $S$ be an empty set.
            \State $i\gets n$
            \While{$i>0$}
                \If{$OPT[i]=OPT[i-1]$}
                    \State $i\gets i-1$ \Comment{$S$ does not contain $i$}
                \Else
                    \State $S\gets S\cup\{i\}$ \Comment{$S$ does contain $i$}
                    \State $i\gets p[i]$ \Comment{$S$ does not contain $p[i]+1,\dots,i-1$}
                \EndIf
            \EndWhile
            \State \Return $S$
            \EndProcedure
        \end{algorithmic}
    \end{enumerate}
\end{itemize}

\subsection{Edit Distance}
\begin{itemize}
    \item Input of two strings of $X=x_1,\dots,x_m$ and $Y=y_1,\dots,y_n$. An ``operation'' is defined as a deletion of a character, or replacement of a character with another character.
    \item Some applications are spelling correction, DNA sequencing, plagiarism detection, etc.
    \item We will solve a generalization of the edit distance problem. We will allow the cost of each operation to be different. So, we will also have as input \begin{itemize}
        \item Cost $d(a)$ of deleting a character $a$.
        \item Cost $r(a,b)$ of replacing a character $a$ with a character $b$ with $r(a,b)=r(b,a)$ and the triangle inequality $r(a,c)\leq r(a,b)+r(b,c)$ holds for any $b.$
    \end{itemize}
    We would try to find the minimum total cost for matching $X$ and $Y.$
    \item Sometimes it is helpful to think of if you have the optimal solution, what is the last operation that was performed? Here, consider the last symbols $x_m$ and $y_n.$ There are three cases:
    \begin{enumerate}[label=(Case \Alph*)]
        \item Deleting $x_m$ and optimally match $x_1,\dots,x_{m-1}$ with $y_1,\dots,y_n.$
        \item Deleting $y_n$ and optimally match $x_1,\dots,x_m$ with $y_1,\dots,y_{n-1}.$
        \item Match $x_m$ with $y_n$ and optimally match $x_1,\dots,x_{m-1}$ with $y_1,\dots,y_{n-1}.$
    \end{enumerate}
    \item Let $E(i,j)$ be the edit distance between $x_1,\dots,x_i$ and $y_1,\dots,y_j.$ Then, the Bellman equation is \begin{align}
        A&=E(i-1,j)+d(x_i)\\
        B&=E(i,j-1)+d(y_j)\\
        C&=E(i-1,j-1)+r(x_i,y_j)\\
        E(i,j)&=\begin{cases}
            \min\{A,B,C\}&\text{if $i,j>0$}\\
            \sum_{k=1}^i d(x_k)&\text{if $i>0$ and $j=0$}\\
            \sum_{k=1}^j d(y_k)&\text{if $j>0$ and $i=0$}\\
            0&\text{if $i=j=0$}
        \end{cases}
    \end{align}
    \item \redbf{Make sure to remember the base cases!}\begin{algorithmic}[1]
        \Procedure {EditDistance}{$X,Y,d,r$}
        \State Let $E$ be an array of size $(m+1)\times(n+1)$ indexed from $0.$
        \State $E(0,0)\gets 0$
        \For{$i=1$ to $m$}
            \State $E(i,0)\gets E(i-1,0)+d(x_i)$
        \EndFor
        \For{$j=1$ to $n$}
            \State $E(0,j)\gets E(0,j-1)+d(y_j)$
        \EndFor
        \For{$i=1$ to $m$}
            \For{$j=1$ to $n$}
                \State $A\gets E(i-1,j)+d(x_i)$
                \State $B\gets E(i,j-1)+d(y_j)$
                \State $C\gets E(i-1,j-1)+r(x_i,y_j)$
                \State $E(i,j)\gets \min\{A,B,C\}$
            \EndFor
        \EndFor
        \State \Return $E(m,n)$
        \EndProcedure
    \end{algorithmic}
    \item This algorithm is $O(mn).$ \redbf{When you need to compute an element (i.e. $E(m,n)),$ make sure its subproblems (i.e. $E(m-1,n-1),E(m,n-1),E(m-1,n)$) are computed!}
    \item To reconstruct the edits, we can obtain \begin{algorithmic}[1]
        \Procedure {RecoverPath}{$E,X,Y,d,r$}
        \State $Ops\gets\emptyset$
        \State $i\gets m; j\gets n$
        \While{$i>0\land j>0$}
            \If{$E(i,j)=d(x_i)+E(i-1,j)$}
                \State $Ops\gets Ops\cup\{\text{Delete $x_i$}\}$
                \State $i\gets i-1$
            \EndIf
            \If{$E(i,j)=d(y_j)+E(i,j-1)$}
                \State $Ops\gets Ops\cup\{\text{Delete $y_j$}\}$
                \State $j\gets j-1$
            \EndIf
            \If{$E(i,j)=r(x_i,y_j)+E(i-1,j-1)$}
                \State $Ops\gets Ops\cup\{\text{Replace $x_i$ with $y_j$}\}$
                \State $i\gets i-1$
                \State $j\gets j-1$
            \EndIf
        \EndWhile
        \EndProcedure
    \end{algorithmic}
    \item The recover path procedure is $O(m+n).$
\end{itemize}

\subsection{0-1 Knapsack}
\begin{itemize}
    \item The knapsack problem is: when given $n$ items, each with a value $v_i>0$ and weight $w_i>0$ and the knapsack can hold a weight $C.$ The goal is to find a subset $S$ of items with maximum total value such that the total weight of $S$ is at most $C.$
    \item 0-1 knapsack refers to you can either take an item or not take an item, nothing in between.
    \item Let $S$ be an optimal knapsack content for items $1,\dots,n$ and capacity $C.$
    \item There are two possible cases for the last item $n.$\begin{enumerate}
        \item If $n\notin S,$ then $S$ is optimal for items $1,\dots,n-1$ and capacity $C.$
        \item If $n\in S,$ then $S$ is optimal for items $1,\dots,n-1$ and capacity $C-w_n.$
    \end{enumerate}
    \item The subproblems we want to solve now have two parameters $n$ and $C,$ with $K(i,c)$ being the value of the optimal knapsack for $1,\dots,i$ and capacity $c.$
    \item The bellman equation is \begin{align}
        K(i,c)&=\begin{cases}
            0&\text{if $i=0$ or $c=0$}\\
            K(i-1,c)&\text{if $w_i>c$}\\
            \max\{K(i-1,c),K(i-1,c-w_i)+v_i\}&\text{if $w_i\leq c$}
        \end{cases}
    \end{align}
    \item We need to compute this by $0$ to $C$ first, then $0$ to $n.$
    \begin{algorithmic}[1]
        \Procedure{Knapsack}{$n,C,w,v$}
        \State For $c=0$ to $C$ do $K(0,c)\gets 0$\Comment{no items}
        \State For $i=0$ to $n$ do $K(i,0)\gets 0$\Comment{no capacity}
        \For{$i=1$ to $n$}
            \For{$c=1$ to $C$}
                \If{$c<w_i$}\Comment{item $i$ is too heavy}
                    \State $K(i,c)\gets K(i-1,c)$
                \Else\Comment{item $i$ can fit, take it or not}
                    \State $K(i,c)\gets \max\{K(i-1,c),K(i-1,c-w_i)+v_i\}$
                \EndIf
            \EndFor
        \EndFor
        \State \Return $K(n,C)$
        \EndProcedure
    \end{algorithmic}
    \item To find $S,$ we can backtrack from $K(n,C)$ to $K(0,0).$ \begin{algorithmic}[1]
        \Procedure{RecoverKnapsack}{$K,n,C,w,v$}
        \State $S\gets\emptyset;c\gets C;i\gets n$
        \While{$i>0$ and $c>0$}
            \If{$K(i,c)=K(i-1,c)$}\Comment{item $i$ is not in $S$}
                \State $i\gets i-1$
            \Else
                \State $S\gets S\cup\{i\}$
                \State $c\gets c-w_i$
                \State $i\gets i-1$
            \EndIf
        \EndWhile
        \State \Return $S$
        \EndProcedure
    \end{algorithmic}
    \item The time complexity is $O(nC).$ The space complexity is also $O(nC).$
    \item Question: is the running time polynomial in the \underline{input size}? \redbf{No!} because the number $C$ is \redbf{exponential in the number of bits} used to represent the input.
    \item This is \bluebf{pseudo-polynomial} because it is polynomial in the input values, not input size.
    \item The problem is NP-hard.
\end{itemize}
\subsubsection{Large Knapsack, Small Items}
\begin{itemize}
    \item Consider that instead of $C,w_1,\dots,w_n$ being small, we were told $v_1,\dots,v_n$ are small. Can we solve this problem in $O(nV)$ where $V=\sum v_i.$ Yes, with a different dynamic programming algorithm.
    \item Define our subproblems $K(i,v)$ to be the minimum capacity of a knapsack that can hold a total value of at least $v$ using items $1,\dots,i.$
    \item For item $i,$ either we should take $i$ to obtain value $v$ or we should not take $i.$
    \begin{itemize}
        \item If we do not choose $i,$ we need $K(i-1,v)$ capacity.
        \item If we do choose $i,$ we need $K(i-1,v-v_i)$ capacity.
    \end{itemize}
    \item The Bellman equation is \begin{align}
        K(i,v)&=\begin{cases}
            0&\text{if $v\leq0$}\\
            \infty&\text{if $v>0,i=0$}\\
            \min\{K(i-1,v),K(i-1,v-v_i)+w_i\}&\text{if $v>0,i>0$}
        \end{cases}
    \end{align}
    \item We should compute this array from $0$ to $V$ first, then $0$ to $n.$
\end{itemize}
\subsubsection{Approximation Algorithms}
\begin{itemize}
    \item Unless $P=NP,$ we cannot hope to solve knapsack in $O(poly(n,\log C,\log V)).$
    \item We can find a good approximate solution in polynomial time. $\forall\varepsilon>0$ we can get a solution that is at least $(1-\varepsilon)$-optimal, in time $O(poly(n,\log C,\log V,1/\varepsilon)).$
\end{itemize}

\subsection{Chain Matrix Multiplication}
\begin{itemize}
    \item Multiplying three matrices $D=ABC,$ then $D=(AB)C=A(BC).$ Although the answers are the same, the cost depends on the multiplication order.
    \item The problem is to find the optimal order of multiplication of a chain of matrices $A_1,\dots,A_n$ and each matrix has dimension $d_{i-1}\times d_i.$
    \item To multiply a $p\times q$ matrix by a $q\times r$ matrix, we obtain a $p\times r$ matrix with $pqr$ multiplications
    \item We define $m(i,j)$ as the minimum number of multiplication to compute $A_i\cdot A_{i+1}\cdot\cdots\cdot A_j.$
    \item Try all the ways to put the outer-level parentheses, then compute \begin{align}
        m(i,j)&=\begin{cases}
            0&\text{if $i=j$}\\
            \min_{i\leq k<j}\{m(i,k)+m(k+1,j)+d_{i-1}d_kd_j\}&\text{if $i<j$}
        \end{cases}
    \end{align}
    \item The naive recursive procedure takes $T(n)=\sum_{k=1}^{n-1}T(k)+T(n-k)+c.$ We can notice that both terms of the sum occur twice so simplify to \begin{align}
        T(n)&=2\sum_{k=1}^{n-1}T(k)+(n-1)c\\
        T(n+1)&=2\sum_{k=1}^{n-1}T(k)+n\\
        T(n+1)-T(n)&=2T(n)+c\\
        T(n+1)&=3T(n)+c\\
        T(n)&=\Theta(3^n)
    \end{align}
    \item This does not work. We can use dynamic programming to solve this problem in $O(n^3).$ We will compute from the bottom up, starting from chains of length $1$ to chains of length $n.$ \begin{algorithmic}[1]
        \Procedure{MatrixChainOrder}{$d_1,\dots,d_n$}
        \For{$i\gets 1$ to $n$}
            \State $m(i,i)\gets 0$
        \EndFor
        \For{$l\gets 1$ to $n-1$}
            \For{$i\gets 1$ to $n-l$}
                \State $j\gets i+l$
                \State $m(i,j)\gets\min_{i\leq k<j}\{m(i,k)+m(k+1,j)+d_{i-1}d_kd_j\}$
            \EndFor
        \EndFor
        \State \Return $m(1,n)$
        \EndProcedure
    \end{algorithmic}
\end{itemize}
\subsection{Bellman-Ford Single Source Shortest Paths}
\begin{itemize}
    \item We want to revisit the shortest paths problem. When the edge weights may be negative, Dijkstra's algorithm does not work. We want an algorithm that works with some negative edge weights.
    \item \redbf{Note that if the graph has a negative cycle, the problem makes no sense.}
    \item \textbf{Claim 1:} With no negative cycles, for every node $t\in V$ there is a shortest path that is \bluebf{simple} (i.e. contains no repeated nodes). Any cycle will have weight $\geq0,$ so just remote it and the path will not be any longer.
    \item Hence, for any node $t\in V,$ there is a shortest path with at most $n-1$ edges.
    \item For $t\neq s,$ consider an oracle that tells us the shortest path from $s$ to $t$ that contains $i$ edges $s\to\cdots\to u\to t.$ Then the shortest path from $s\to u$ has at most $i-1$ edges.
    \item We define our subproblems as $OPT(i,t)$ as the length of a shortest $s\to t$ path using \textbf{at most} $i$ edges. So, $OPT(i,t)=OPT(i-1,u)+w(u,t)$ for some $u\in V.$
    \item So, \begin{align}
        OPT(i,t)&=\begin{cases}
            0&\text{if $i=0$ and $t=s$}\\
            \infty&\text{if $i=0$ and $t\neq s$}\\
            \min_{u\in V}\{OPT(i-1,u)+w(u,t)\}&\text{if $i>0$}
        \end{cases}
    \end{align}
    Note that part of the $\min$ is the case where $u=t.$ In that case $w(t,t)=0$ so this case would be $OPT(i-1,t).$
    \item Then, $OPT(n-1,t)$ is the length of the shortest path from $s$ to $t.$ The naive implementation is $O(n^3).$ \begin{algorithmic}[1]
        \State $OPT(0,s)\gets0$
        \State for all $t\neq s,\,OPT(0,t)\gets\infty$
        \For{$i=1$ to $n-1$}
            \For{all $t\in V$}
                \State $OPT(i,t)\gets\min_{u\in V}\{OPT(i-1,u)+w(u,t)\}$
            \EndFor
        \EndFor
        \State \Return $OPT(n-1,t)$
    \end{algorithmic}
    \item We can improve this by not checking all the nodes, but only nodes that have an edge going \bluebf{into} $t$ as well as $t$ to itself. This is like reversing an adjacency list. Hence, instead of minimizing over all $u\in V,$ we only minimize over $u\in\{v\in V|w(u,t)<\infty\}.$ This way the runtime is $O(nm).$
    \item We do not have to store the matrix, which is $O(n^2)$ space. As each row only depends on the previous row, we can just store the two rows reducing the space complexity to $O(n).$
    \item To recover the path, we keep track of the current predecessor. Each time our path decreases, we update the predecessor.
\end{itemize}
\subsubsection{Detecting Negative Cycles}
\begin{itemize}
    \item \textbf{Claim 2:}\begin{align}
        \Big(\forall t\in V,\,OPT(k,t)=OPT(k-1,t)\Big)\implies\Big(\forall t\in V,\,OPT(k,t)=OPT(k+1,t)\Big).
    \end{align}
    \item Using the bellman equation, \begin{align}
        OPT(k+1,t)&=\min_{u\in V}\{OPT(k,u)+w(u,t)\}\\
        &=\min_{u\in V}\{OPT(k-1,u)+w(u,t)\}\\
        &=OPT(k,t).
    \end{align}
    \item Even if we don't care about negative cycles, this may help us stop early when we go through one iteration with no improvement.
    \item If $G$ has no negative cycle reachable from $s,$ \textbf{iff} $\forall t\in V,\,OPT(n,t)=OPT(n-1,t).$
    \item Proof $[\implies]:$ If there are no negative cycles reachable from $s,$ there exists a shortest path from $s$ to $t$ with at most $n-1$ edges. Hence, $OPT(n,t)=OPT(n-1,t).$
    \item Proof $[\impliedby]:$ Suppose $\forall t\in V,\,OPT(n,t)=OPT(n-1,t).$ Then for any $n'\geq n,OPT(n',t)=OPT(n-1,t).$ Suppose for contradiction that there is a negative cycle reachable from $s.$ Then, we can find a path from $s\to t$ for some $t$ in the negative cycle. We will repeat the negative cycle enough time so that $OPT(n',t)<OPT(n-1,t).$ This is a contradiction.
    \item Hence, to detect negative cycles we run Bellman Ford one extra time and check if $OPT(n,t)=OPT(n-1,t).$ If there is some $t\in V$ such that $OPT(n,t)<OPT(n-1,t),$ then there is a negative cycle reachable from $s.$
    \item \textbf{To detect all negative cycles}, we add a new node $s$ to the graph and connect it to all other nodes with weight $0.$ Then, we run Bellman Ford on this new graph. If there is a negative cycle it is reachable from $s.$
    \item \textbf{Claim 4:} Suppose that $OPT(n,t)\neq OPT(n-1,t)$ for some node $t.$ Then let $P$ be an $s\to t$ path of length $OPT(n,t)$ and with at most $n$ edges then \begin{enumerate}
        \item $P$ contains a cycle.
        \item Every cycle of $P$ has a negative length.
    \end{enumerate}
    \item $1$ is trivial, as the path has $n$ edges and $n+1$ nodes. By pigeonhole principle, there at least one node is included twice because $G$ only has $n$ nodes.
    \item Suppose $P=s\to\cdots\to v\to\cdots\to v\to\cdots\to t.$ Suppose by contradiction the path $v\to\cdots\to v$ has non-negative weight. Then, let $P'=s\to\cdots\to v\to\cdots\to t.$ The length of $P'$ is at most the length of $P$ but has at most $n-1$ edges. Hence, $OPT(n,t)\geq len(P')\geq OPT(n-1,t)$ which is a contradiction.
\end{itemize}
\subsection{Floyd-Warshall All-Pairs Shortest Paths}
\begin{itemize}
    \item Suppose we are given a graph $G=(V,E)$ with no negative length cycles. We want the shortest path from each node $s$ to each node $t.$ 
    \item If we use Bellman Ford, it will run in $O(n^2m)$ and if the graph is dense $m=O(n^2)$ then the runtime is $O(n^4),$ which is bad.
    \item $V=\{1,2,\cdots,n\}.$ We define $P$ as a $i\to^kj$ path if every intermediate node in $P$ is $\leq k.$ (i.e. $P=1\to 5\to 2\to 7$ is a $1\to^57$ path but not a $1\to^67$ path)
    \item Hence, we will define $OPT(i,j,k)$ as the length of the shortest (simple) $i\to^kj$ path. Consider the following two cases \begin{enumerate}
        \item $k$ is not an intermediate node of $P.$ Then, $OPT(i,j,k)=OPT(i,j,k-1).$
        \item $k$ is an intermediate node then $OPT(i,j,k)=OPT(i,k,k-1)+OPT(k,j,k-1).$
    \end{enumerate}
    \item The bellman equation is \begin{align}
        OPT(i,j,0)&=\begin{cases}
            0&\text{if }i=j\\
            w(i,j)&\text{if }i\to j\text{ is an edge}\\
        \end{cases}\\
        OPT(i,j,k)&=\min\{OPT(i,j,k-1),OPT(i,k,k-1)+OPT(k,j,k-1)\}
    \end{align}
    \begin{algorithmic}[1]
        \For{$i=1$ to $n$}
            \For{$j=1$ to $n$}
                \If{$i=j$}
                    \State $OPT(i,j,0)=0$
                \Else
                    \State $OPT(i,j,0)=w(i,j)$
                \EndIf
            \EndFor
        \EndFor
        \For{$k=1$ to $n$}
            \For{$i=1$ to $n$}
                \For{$j=1$ to $n$}
                    \If{$OPT(i,j,k-1)>OPT(i,k,k-1)+OPT(k,j,k-1)$}
                        \State $OPT(i,j,k)=OPT(i,k,k-1)+OPT(k,j,k-1)$
                    \Else
                        \State $OPT(i,j,k)=OPT(i,j,k-1)$
                    \EndIf
                \EndFor
            \EndFor
        \EndFor
    \end{algorithmic}
    \item The runtime complexity is $O(n^3).$ Naively, the space complexity is also $O(n^3),$ but note that row $k$ only depends on row $k-1,$ so we can reduce the space complexity to $O(n^2).$
    \item To recover the path, each node need to store for every source, what is the previous node in the shortest path.
\end{itemize}
\subsubsection{Transitive Closure Graph}
\begin{itemize}
    \item The \bluebf{transitive closure} of a graph $G^*=(V,E^*),$ where \begin{align}
        E^*=\{(u,v)|G\text{ has an edge from $u\to v$}\}.
    \end{align}
    \item To compute the transitive closure of $G(V,E),$ we can use Floyd Warshall after changing all edge weights to $i.$ If $OPT(u,v,n)\neq\infty\iff G$ has a $u\to v$ path $\iff G^*$ has an edge from $u\to v.$
    \item A better way is to modify the Floyd Warshall (replace $+$ with $\land$ and $\min$ with $\lor$)
\end{itemize}
\subsubsection{Detecting Negative Cycles}
\begin{itemize}
    \item \textbf{Claim 4:} If $G$ has a negative cycle, then $\exists u\in V,OPT(u,u,n)<0$ 
\end{itemize}

\section{Network Flow}
\begin{itemize}
    \item A \bluebf{flow network} $\mathcal F=(G,s,t,c)$ where $G=(V,E)$ is a directed graph and $s\in V$ is the start node and have no incoming edge, $t\in V$ is the terminal node and have no outgoing edge, and $c:E\to\R_+$ is the capacity function.
    \item A \bluebf{flow} $f:E\to\R$ that respects the following constraints:\begin{enumerate}
        \item $\forall\,e\in E,0\leq f(e)\leq c(e)$
        \item $\forall\,v\in V\setminus\{s,t\},\sum_{e\in E^+(v)}f(e)-\sum_{e\in E^-(v)}f(e)=0$
    \end{enumerate}
    \item The \bluebf{value} of a flow $f$ is $v(f)=f^{out}(s)=f^{in}(t).$
    \item The \textbf{max-flow problem} is: given a flow network $\mathcal F$ to find the flow $f$ with maximum $v(f).$
    \item Given a flow $f,$ the \bluebf{residual graph} is the graph $G_f=(V,E_f)$ where for each edge $(u,v)\in E,$ $E_f$ has at most two edges \begin{itemize}
        \item Forward edge: $e=(u,v)$ with residual capacity $c(e)-f(e)>0$
        \item Reverse edge: $e'=(v,u)$ with residual capacity $f(e')>0$) 
    \end{itemize}
    Note that if an edge is saturated, the edge is not included in $E_f.$
    \item An \bluebf{augmenting path} is a path $P$ in $G_f$ from $s$ to $t.$ The \bluebf{bottleneck} of $P$ is the smallest residual capacity of any edge in $P.$
    \item To augment $f,$ we can add the bottleneck $x$ of an augmenting path to $f$ to obtain $f'$ by \begin{itemize}
        \item For each forward edge of $P,$ increase the flow by $x.$
        \item For each reverse edge of $P,$ decrease the flow by $x.$
    \end{itemize}
    \item We will first argue that $v(f')>v(f).$ As there is no edge into $s,$ we only have forward edges from $s,$ then $v(f')=f'^{out}(s)=f^{out}(s)+x>v(f)$
    \item We also need to argue we do not violate the capacity constraints. This is trivial.
    \item Finally, we must argue that flow conservation is preserved. At each node $e,$ we must consider $4$ cases. \begin{enumerate}
        \item If $v$ is not on $P,$ then nothing changes.
        \item The edge entering $v$ and the edge leaving $v$ in the augmenting path $P$ are both forward edges. Then both $f^{out}(v)$ and $f^{in}(v)$ increase by $x.$
        \item If one is forward and one is reverse, then $f^{in}(v)$ and $f^{out}(v)$ are unchanged.
        \item If both are reverse, then both $f^{in}(v)$ and $f^{out}(v)$ decrease by $x.$
    \end{enumerate}
\end{itemize}
\subsection{Ford-Fulkerson Algorithm}
\begin{algorithmic}[1]
    \Procedure{FordFulkerson}{$\mathcal F=(G,s,t,c)$}
        \Foreach{edge $(u,v)$ of $G$} 
            \State $f(u,v)\leftarrow 0$
        \EndForeach
        \State construct $G_f$
        \While{$G_f$ has an $s\to t$ path}
            \State $P\gets$ any simple $s\to t$ path in $G_f$
            \State augment$(f,P)$
            \State update $G_f$
        \EndWhile
        \Return $f$
    \EndProcedure
\end{algorithmic}


\end{document}