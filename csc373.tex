\documentclass[a4paper]{article}
\usepackage{stdtemplate}

\title{CSC373 Algorithms}
\author{Jonah Chen}
\date{}
\begin{document}
\maketitle
\sffamily
\section{Divide and Conquer}
\begin{itemize}
    \item Divide and Conquer algorithm:
    \begin{enumerate}
        \item Divide problem of size $n$ into $a$ smaller subproblems of size $n/b$ each
        \item Recursively solve each subproblem
        \item Combine the subproblem solutions into the solution of the original problem
    \end{enumerate}
    \item Runtime: $T(1)=c$ and $T(n)=a T(n/b) + cn^d$ for $n>1$
    \item Master Theorem: $T(n)$ depends on relation between $a$ and $b^d$. \begin{equation}
        \begin{cases}
            a<b^d: T(n)=\Theta(n^d)\\
            a=b^d: T(n)=\Theta(n^d\log n)\\
            a>b^d: T(n)=\Theta(n^{\log_b a})
        \end{cases}
    \end{equation}
    \begin{itemize}
        \item Note that the running time does not depend on the constant $c$
        \item In many algorithms $d=1$ (combining take linear time)
    \end{itemize}
    \item Examples:
    \begin{itemize}
        \item Merge sort --- sorting array of size $n$ ($a=2,\,b=2,\,d=1\rightarrow a=b^d$) so $T(n)=\Theta(n\log n)$
        \item Binary search --- searching sorted array of size $n$ ($a=1,\,b=2,\,d=0\rightarrow a=b^d$) so $T(n)=\Theta(\log n)$
    \end{itemize}
\end{itemize}
\subsection{Karatsuba Multiplication}
\begin{itemize}
    \item \textbf{Add} two binary $n$-bit numbers naively takes $\Theta(n)$ time
    \item \textbf{Multiply} two binary $n$-bit numbers naively takes $\Theta(n^2)$ time
    \item Divide and Conquer approaches \begin{enumerate}
        \item Multiply $x$ and $y$. We can divide them into two parts\begin{align}
            x &= x_1\cdot 2^{n/2} + x_0\\
            y &= y_1\cdot 2^{n/2} + y_0\\
            x\cdot y &= x_1\cdot y_1\cdot 2^n + (x_1\cdot y_0 + x_0\cdot y_1)\cdot 2^{n/2} + x_0\cdot y_0 \label{eq:mul}
        \end{align}
        \begin{itemize}
            \item $T(n)=4T(n/2)+cn; T(1)=c$
            \item $a=4,b=2,d=1$ Master Theorem case 3, $T(n)=\Theta(n^{\log_2 4})=\Theta(n^2)$.
            \item This is the same complexity of the naive approach, making this approach useless.
        \end{itemize}
        
        \item Reconsider \eqref{eq:mul}, we may rewrite $(x_1\cdot y_0+x_0\cdot y_1)$ as $(x_1+x_0)\cdot(y_1+y_0)-x_1\cdot y_1-x_0\cdot y_0$
        \begin{equation}
            x\cdot y = x_1\cdot y_1\cdot 2^n + ((x_1+x_0)\cdot(y_1+y_0)-x_1\cdot y_1-x_0\cdot y_0)\cdot 2^{n/2} + x_0\cdot y_0
        \end{equation}
         
        \begin{itemize}
            \item $T(n)=3T(n/2)+cn; T(1)=c$
            \item $a=3,b=2,d=1$, Master Theorem case 3, $T(n)=\Theta(n^{\log_2 3})=\Theta(n^{\log_2 3})\approx\Theta(n^{1.585})$
            \item Minor issue: a carry may increase $x_1+x_0$ and $y_1+y_0$ to $\frac{n}{2} + 1$. We can easily prove this by isolating the carry bit and reevaluating the complexity.
        \end{itemize}
    \end{enumerate}
    \item To deal with integers which doesn't have a power of 2 number of bits, we can pad the numbers with 0s to make them have a power of 2 number of bits. This may at most increase the complexity by 3x.
    \item 1971: $\Theta(n\cdot\log n\cdot \log\log n)$
    \item 2019: Harvey and van der Hoeven $\Theta(n\log n)$. We do not know if this is optimal.
\end{itemize}
\subsection{Strassen's MatMul Algorithm}
\begin{itemize}
    \item Let $A$ and $B$ be two $n\times n$ matrices (for simplicity $n$ is a power of 2), we want to compute $C=AB$.
    \item The naive approach takes $\Theta(n^3)$ time.
    \begin{enumerate}
        \item Divide $A$ and $B$ into $4$ submatrices of size $\frac{n}{2}\times\frac{n}{2}$ each
        \begin{equation}
            A = \begin{bmatrix}
                A_{1} & A_{2}\\
                A_{3} & A_{4}
            \end{bmatrix}.
        \end{equation}
        Then, $C$ can be calculated with
        \begin{align}
            C_1 &= A_1B_1 + A_2B_3\\
            C_2 &= A_1B_2 + A_2B_4\\
            C_3 &= A_3B_1 + A_4B_3\\
            C_4 &= A_3B_2 + A_4B_4
        \end{align}
        \begin{itemize}
            \item $T(n)=8T(n/2)+cn^2; T(1)=c$
            \item $a=8,b=2,d=2$, case 3 $T(n)=\Theta(n^{\log_2 8})=\Theta(n^3)$
        \end{itemize}
        \item \textbf{Idea:} Compute $C_1,C_2,C_3,C_4$ with only $7$ multiplications, not 8.
        \begin{align}
            M_1 &= (A_2-A_4)(B_3+B_4)\\
            M_2 &= (A_1+A_4)(B_1+B_4)\\
            M_3 &= (A_1-A_3)(B_1+B_2)\\
            M_4 &= (A_1+A_2)B_4\\
            M_5 &= A_1(B_2-B_4)\\
            M_6 &= A_4(B_3-B_1)\\
            M_7 &= (A_3+A_4)B_1
        \end{align}
        With these we can compute $C_1,C_2,C_3,C_4$ with only additions of the $M$ matrices.
        \begin{align}
            C_1 &= M_1+M_2-M_4+M_6\\
            C_2 &= M_4+M_5\\
            C_3 &= M_6+M_7\\
            C_4 &= M_2-M_3+M_5+M_7
        \end{align}
        \begin{itemize}
            \item $T(n)=7T(n/2)+cn^2; T(1)=c$
            \item $a=7,b=2,d=2$, case 3 $T(n)=\Theta(n^{\log_2 7})=\Theta(n^{\log_2 7})\approx\Theta(n^{2.807})$
        \end{itemize}
    \end{enumerate}
    \item If $n$ is not a power of 2, we zero-pad the matrices to have $n$ as a power of two. This may increase the complexity by at most a factor of 7.
\end{itemize}

\subsection{Median of Unsorted Arrays}
\begin{itemize}
    \item For an unsorted array $A$, we can find the average, max, min, sum, etc. in linear time.
    \item The trivial algorithm is to sort $A$ then get the median. That takes $O(n\log n)$ time.
    \item We will solve a more general problem: Find the $k^{th}$ smallest element in $A$. (e.g. $A=5,2,6,7,4$, $\mathrm{Select}(A,1)=2, \mathrm{Select}(A,4)=6$)
    \item if $|A|=1$, then return $A[1]$. Otherwise find a splitter $s$ in arbitrary element of $A$. Partition $A$ into $A^+$ and $A^-$, then divide 
    \item $T(n)=T(\max(|A^-|,|A^+|))+cn=T(\max(i-1,n-i))+cn$.
    \item Worst case: $T(n)=T(n-1)+cn=\Theta(n^2)$
    \item Best case: $T(n)=T(n/2)+cn=\Theta(n)$. Suppose $b>1$, by the Master Theorem $T(n)=T(n/b)+cn=\Theta(n)$.
\end{itemize}

We define $s$ is a good splitter if $s$ is greater than $1/4$ of the elements of $A$ and less than $1/4$ of the elements of $A$. We can make the following observation:
\begin{enumerate}
    \item With this splitter, we will reduce the size to at most $3n/4$.
    \item Half the elements are good splitters.
\end{enumerate}

We should select splitter $s$ uniformly at random.
\begin{itemize}
    \item $P(\text{splitter is good})=\frac{1}{2}$
    \item $P(\text{splitter is bad})=\frac{1}{2}$
    \item We can show that the expected number of trials (splitter selections) until obtaining a good splitter is 2.
\end{itemize}
\subsubsection{Expected Runtime}
\begin{equation}
    \underbrace{n_0\to n_1\to n_2}_{\text{Phase 0, size}\leq n}\to \underbrace{n_3\to n_4}_{\text{Phase 1, size}\leq \frac{3}{4}n}\to \underbrace{n_5\to n_6}_{\text{Phase 2, size}\leq \frac{3}{4}^2 n}\to\dots
\end{equation}
\begin{itemize}
    \item Phase $j$: input size $\leq(\frac{3}{4})^j\,n$
    \item Random variable $y_j = $\# of recursive calls in phase $j$. Note that $E[y_j]=2$.
    \item Random variable $x_j = $\# of steps to all the recursive calls in phase $j$.
    \item Total number of steps is $x=x_0+x_1+x_2+\dots$.
    \item We can compute $E[x]=E[x_0]+E[x_1]+E[x_2]+\dots$.
    \begin{align}
        x_j&\leq c y_j \frac{3}{4}^j n\\
        E[x_j]&\leq c E[y_j] \frac{3}{4}^j n\leq 2c\frac{3}{4}^j n\\
        E[x]&=\sum_j E[x_j]\leq \sum_{j=1}^\infty 2c\frac{3}{4}^j n=\frac{2c}{1-\frac{3}{4}}n=8cn=\Theta(n)
    \end{align}
\end{itemize}

\subsubsection{Deterministic Algorithm}
\begin{itemize}
    \item If $|A|\leq 5$ then we sort $A$ and return the $k^{th}$ smallest.
    \item Otherwise, partition $A$ into $n/5$ groups of size $5$ each, then find the median of each group (constant time) and store in list $M$. This takes linear time.
    \item Select the median of $M$ with the Select algorithm, this is a good splitter.
    \item the worst case running time is $T(n)=T(\lceil\frac{n}{5}\rceil)+T(\lfloor{\frac{3n}{4}}\rfloor)+cn$.
    \item This recursive relation cannot be solved by the Master Theorem. We can prove using induction that $T(n)<20cn$.
\end{itemize}
\textbf{Question: Why groups of 5?}
\begin{itemize}
    \item With groups of 5, the total size of subproblems: $\frac{n}{5}+\frac{3n}{4}=\frac{19n}{20}<n$
    \item With groups of 3, the total size of subproblems: $\frac{n}{3}+\frac{3n}{4}=\frac{13n}{12}>n$, not sufficient.
    \item So group size of $5,7,9,11,\dots$ would also work.
\end{itemize}

\section{Closest Pair of Points}
\begin{itemize}
    \item Problem: Given a set of $n$ points, find the pair of points that are the closest in $O(n\log n)$.
\end{itemize}
\subsection{Closest Pair in 2D}
\begin{itemize}
    \item Divide: points roughly in half by drawing vertical line on midpoint
    \item Conquer: Find closest pair on each half, recursively.
    \item Combine: Find the closest pair $(p,q)$, $p\in L$, $q\in R$. However, there may be $\Theta(n^2)$ pairs.
\end{itemize}
\begin{itemize}
    \item Claim: Let $p=(x_p,y_p)\in B_L,q=(x_q,y_q)\in B_R$ with $y_p\leq y_q$. If $d(p,q)<\delta$ then there are at most \textbf{six} other points (x,y) in $B$ such that $y_p\leq y\leq y_q$.
    \item Proof:
    \item $S_L=\{p'=(x,y):p'\neq p\in B_L\land y_p\leq y\leq y_q\}$ (other points on the left of the middle)
    \item $S_R=\{p'=(x,y):p'\neq q\in B_R\land y_p\leq y\leq y_q\}$ (other points on the right of the middle)
    \item Assume by contradiction that $|S_L\cup S_R|\geq 7$. WLOG assume $|S_L|\geq 4$.
    \item In a $\delta\times\delta$ square there are at least $4+1=5$ points. Divide the square into 4 smaller squares, by Pigeonhole Principle, there is a square with at least 2 points, whose distance is at most $\delta/\sqrt2$. This contradicts the assumption that the closest pair on the left is at most $\delta$.
\end{itemize}
\begin{itemize}
    \item Then, we can sort everything in the $y$ axis, and check the next seven points by the $y$ coordinate for the minimum distance. This takes linear time.
    \item We only need to modify the combine step in the algorithm so it's $\Theta(n)$ runtime.
\end{itemize}

\begin{algorithm}
\caption{Closest Pair in 2D}
\begin{algorithmic}[1]
\Procedure{ClosestPair}{$P$}
\State $P_x:=\text{the list of points in }P\text{ sorted by x-coordinate}$
\State $P_y:=\text{the list of points in }P\text{ sorted by y-coordinate}$
\EndProcedure
\Procedure{RCP}{$P_x,P_y$}
\If{$|P_x|\leq 3$}
\Return{$\text{brute force}(P_x)$}
\EndIf
\State $L_x:=\text{the first half of }P_x;R_x:=\text{the second half of }P_x$
\State $m:=(\text{max x-coordinate of }L_x+\text{min x-coordinate of }R_x)/2$
\State $L_y:=\text{sublist of }P_y\text{ with points in }L_x$
\State $R_y:=\text{sublist of }P_y\text{ with points in }R_x$
\State $(p_L,q_L):=\text{RCP}(L_x,L_y);(p_R,q_R):=\text{RCP}(R_x,R_y)$
\State $\delta:=\min\{d(p_L,q_L),d(p_R,q_R)\}$
\If {$\delta=d(p_L,q_L)$}
\State $p:=p_L;q:=q_L$
\Else
\State $p:=p_R;q:=q_R$
\EndIf
\State $B:=\text{sublist of }P_y\text{ with points in }[m-\delta,m+\delta]$
\Foreach {$p$ in $B$}
\Foreach {next seven $q$ after $p$ in $B$}
\If {$d(p,q)<d(p^*,q^*)$} $(p^*,q^*):=(p,q)$
\EndIf
\EndForeach
\EndForeach
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{itemize}
    \item So $T(n)=2T(\frac{n}{2})+cn$, which is $O(n\log n)$.
\end{itemize}

\section{Greedy Algorithms}
\begin{itemize}
    \item There is an optimization problem: given an input, find a solution that minimize/maximize an objective function $f$ under some constraint.
    \item Build the solution incrementally in stages
    \item At each stage, extend the solution greedily and irrevocably.
    \item For some problems this gives optimal solutions (i.e. MST), but for other problems it does not.
    \item The order of the stages is very important.
\end{itemize}

\subsection{Interval Scheduling}
\begin{itemize}
    \item Input: $n$ intervals, with interval $j$ starts at time $s_j$ and finishes at time $f_j$.
    \item Output: maximum-size set of intervals that do not overlap
    \item Naive algorithm is to try each subset of $n$ intervals by brute force $O(2^n)$, way too slow.
    \item Greedy algorithm sorts interval in some order, then if it doesn't overlap then add it to the solution.
    \item \textbf{What order gives the biggest feasible set?}
    \begin{enumerate}
        \item Increasing start time: $s_j$
        \item Increasing finish time: $e_j$
        \item Shortest interval: $e_j-s_j$
        \item Fewest conflicts
    \end{enumerate}
    \item The question is which one is optimal? Option 1 has a clear counterexample with one very long interval that overlaps each other interval. Option 3 is also not optimal using a short interval between two long interval. Option 4 is not optimal (not very clear counterexample).
    \item Option 2 is optimal. The intuition is that chosing these intervals first will leave the most time for the rest of the intervals.
    \item To find if the interval is compatible, we just need to check if the start time of the new interval is greater than the finish time of the latest scheduled interval.
\end{itemize}
\subsubsection{Proof of optimality}
\begin{itemize}
    \item Suppose for contradiction that this greedy algorithm is not optimal
    \item Say greedy selects interval $i_1,\dots,i_k$ sorted by increasing finish time.
    \item Suppose the optimal schedule $j_1,\dots,j_m$ has $m>k$ intervals, and sort by increasing finish time. Consider an optimal schedule that can match the greedy $j_1=i_1,\dots,j_r=i_r$ for the greatest possible $r$.
    \item By the nature of the greedy algorithm, then $i_{r+1}$ finishes the earliest amongst the compatible intervals remaining. So consider the schedule $S:i_1,\dots,i_r,\mathbf{i_{r+1}},j_{r+2},\dots,j_m$. 
    \item This is also optimal (contains $m$ intervals) but matches the greedy algorithm by one more position. This is a contradiction.
\end{itemize}

\subsection{Interval Partitioning}
\begin{itemize}
    \item Motivation: given a set of lecture time intervals, schedule them into as few classrooms as possible.
    \item Input: $n$ intervals, interval $j$ starts at $s_j$ and finishes at $f_j$.
    \item Output: group interval into fewest partitions such that intervals in each partition are compatible.
    \item An idea is to find the maximum set of compatible intervals using the previous algorithm. This doesn't work.
    \item We can try the same orders as the previous problem, but only earliest start time is optimal in this case.
    \item To implement it efficiently, we will use a heap with increase-key operation.
\end{itemize}
\subsubsection{Proof of optimality}
\begin{itemize}
    \item We define the \textbf{depth} at time $t$ as the number of intervals that contain time $t$. The \textbf{maximum depth} $d_{max}$ is the maximum depth over all times.
    \item Clearly, the number of partitions needed is at least $d_{max}$. We will show that this greedy algorithm create only $d_{max}$ partitions.
    \item Let $d$ be the number of partition the greedy algorithm opened.
    \item Partition $d$ was created because there was an interval $j$ that overlaps with some previously scheduled interval in each of the $d-1$ other partitions.
    \item This means that for $d-1$ intervals, their start times are all before $s_j$ and their finish times are all after $f_j$ (otherwise $j$ must be compatible). Hence, the depth at $s_j$ is exactly $d$.
    \item Thus, $d_{max}\geq d$ so the greedy algorithm is optimal.
    \item Warning: this proof rely on the fact that the start time
\end{itemize}


\end{document}