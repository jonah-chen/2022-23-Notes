\documentclass[a4paper,12pt]{article}
\usepackage{stdtemplate}

\title{CSC373 Algorithms}
\author{Jonah Chen}
\date{}
\begin{document}
\maketitle
\sffamily
\section{Divide and Conquer}
\begin{itemize}
    \item Divide and Conquer algorithm:
    \begin{enumerate}
        \item Divide problem of size $n$ into $a$ smaller subproblems of size $n/b$ each
        \item Recursively solve each subproblem
        \item Combine the subproblem solutions into the solution of the original problem
    \end{enumerate}
    \item Runtime: $T(1)=c$ and $T(n)=a T(n/b) + cn^d$ for $n>1$
    \item Master Theorem: $T(n)$ depends on relation between $a$ and $b^d$. \begin{equation}
        \begin{cases}
            a<b^d: T(n)=\Theta(n^d)\\
            a=b^d: T(n)=\Theta(n^d\log n)\\
            a>b^d: T(n)=\Theta(n^{\log_b a})
        \end{cases}
    \end{equation}
    \begin{itemize}
        \item Note that the running time does not depend on the constant $c$
        \item In many algorithms $d=1$ (combining take linear time)
    \end{itemize}
    \item Examples:
    \begin{itemize}
        \item Merge sort --- sorting array of size $n$ ($a=2,\,b=2,\,d=1\rightarrow a=b^d$) so $T(n)=\Theta(n\log n)$
        \item Binary search --- searching sorted array of size $n$ ($a=1,\,b=2,\,d=0\rightarrow a=b^d$) so $T(n)=\Theta(\log n)$
    \end{itemize}
\end{itemize}
\subsection{Karatsuba Multiplication}
\begin{itemize}
    \item \bluebf{Add} two binary $n$-bit numbers naively takes $\Theta(n)$ time
    \item \bluebf{Multiply} two binary $n$-bit numbers naively takes $\Theta(n^2)$ time
    \item Divide and Conquer approaches \begin{enumerate}
        \item Multiply $x$ and $y$. We can divide them into two parts\begin{align}
            x &= x_1\cdot 2^{n/2} + x_0\\
            y &= y_1\cdot 2^{n/2} + y_0\\
            x\cdot y &= x_1\cdot y_1\cdot 2^n + (x_1\cdot y_0 + x_0\cdot y_1)\cdot 2^{n/2} + x_0\cdot y_0 \label{eq:mul}
        \end{align}
        \begin{itemize}
            \item $T(n)=4T(n/2)+cn; T(1)=c$
            \item $a=4,b=2,d=1$ Master Theorem case 3, $T(n)=\Theta(n^{\log_2 4})=\Theta(n^2)$.
            \item This is the same complexity of the naive approach, making this approach useless.
        \end{itemize}
        
        \item Reconsider \eqref{eq:mul}, we may rewrite $(x_1\cdot y_0+x_0\cdot y_1)$ as $(x_1+x_0)\cdot(y_1+y_0)-x_1\cdot y_1-x_0\cdot y_0$
        \begin{equation}
            x\cdot y = x_1\cdot y_1\cdot 2^n + ((x_1+x_0)\cdot(y_1+y_0)-x_1\cdot y_1-x_0\cdot y_0)\cdot 2^{n/2} + x_0\cdot y_0
        \end{equation}
         
        \begin{itemize}
            \item $T(n)=3T(n/2)+cn; T(1)=c$
            \item $a=3,b=2,d=1$, Master Theorem case 3, $T(n)=\Theta(n^{\log_2 3})=\Theta(n^{\log_2 3})\approx\Theta(n^{1.585})$
            \item Minor issue: a carry may increase $x_1+x_0$ and $y_1+y_0$ to $\frac{n}{2} + 1$. We can easily prove this by isolating the carry bit and reevaluating the complexity.
        \end{itemize}
    \end{enumerate}
    \item To deal with integers which doesn't have a power of 2 number of bits, we can pad the numbers with 0s to make them have a power of 2 number of bits. This may at most increase the complexity by 3x.
    \item 1971: $\Theta(n\cdot\log n\cdot \log\log n)$
    \item 2019: Harvey and van der Hoeven $\Theta(n\log n)$. We do not know if this is optimal.
\end{itemize}
\subsection{Strassen's MatMul Algorithm}
\begin{itemize}
    \item Let $A$ and $B$ be two $n\times n$ matrices (for simplicity $n$ is a power of 2), we want to compute $C=AB$.
    \item The naive approach takes $\Theta(n^3)$ time.
    \begin{enumerate}
        \item Divide $A$ and $B$ into $4$ submatrices of size $\frac{n}{2}\times\frac{n}{2}$ each
        \begin{equation}
            A = \begin{bmatrix}
                A_{1} & A_{2}\\
                A_{3} & A_{4}
            \end{bmatrix}.
        \end{equation}
        Then, $C$ can be calculated with
        \begin{align}
            C_1 &= A_1B_1 + A_2B_3\\
            C_2 &= A_1B_2 + A_2B_4\\
            C_3 &= A_3B_1 + A_4B_3\\
            C_4 &= A_3B_2 + A_4B_4
        \end{align}
        \begin{itemize}
            \item $T(n)=8T(n/2)+cn^2; T(1)=c$
            \item $a=8,b=2,d=2$, case 3 $T(n)=\Theta(n^{\log_2 8})=\Theta(n^3)$
        \end{itemize}
        \item \textbf{Idea:} Compute $C_1,C_2,C_3,C_4$ with only $7$ multiplications, not 8.
        \begin{align}
            M_1 &= (A_2-A_4)(B_3+B_4)\\
            M_2 &= (A_1+A_4)(B_1+B_4)\\
            M_3 &= (A_1-A_3)(B_1+B_2)\\
            M_4 &= (A_1+A_2)B_4\\
            M_5 &= A_1(B_2-B_4)\\
            M_6 &= A_4(B_3-B_1)\\
            M_7 &= (A_3+A_4)B_1
        \end{align}
        With these we can compute $C_1,C_2,C_3,C_4$ with only additions of the $M$ matrices.
        \begin{align}
            C_1 &= M_1+M_2-M_4+M_6\\
            C_2 &= M_4+M_5\\
            C_3 &= M_6+M_7\\
            C_4 &= M_2-M_3+M_5+M_7
        \end{align}
        \begin{itemize}
            \item $T(n)=7T(n/2)+cn^2; T(1)=c$
            \item $a=7,b=2,d=2$, case 3 $T(n)=\Theta(n^{\log_2 7})=\Theta(n^{\log_2 7})\approx\Theta(n^{2.807})$
        \end{itemize}
    \end{enumerate}
    \item If $n$ is not a power of 2, we zero-pad the matrices to have $n$ as a power of two. This may increase the complexity by at most a factor of 7.
\end{itemize}

\subsection{Median of Unsorted Arrays}
\begin{itemize}
    \item For an unsorted array $A$, we can find the average, max, min, sum, etc. in linear time.
    \item The trivial algorithm is to sort $A$ then get the median. That takes $O(n\log n)$ time.
    \item We will solve a more general problem: Find the $k^{th}$ smallest element in $A$. (e.g. $A=5,2,6,7,4$, $\mathrm{Select}(A,1)=2, \mathrm{Select}(A,4)=6$)
    \item if $|A|=1$, then return $A[1]$. Otherwise find a splitter $s$ in arbitrary element of $A$. Partition $A$ into $A^+$ and $A^-$, then divide 
    \item $T(n)=T(\max(|A^-|,|A^+|))+cn=T(\max(i-1,n-i))+cn$.
    \item Worst case: $T(n)=T(n-1)+cn=\Theta(n^2)$
    \item Best case: $T(n)=T(n/2)+cn=\Theta(n)$. Suppose $b>1$, by the Master Theorem $T(n)=T(n/b)+cn=\Theta(n)$.
\end{itemize}

We define $s$ is a good splitter if $s$ is greater than $1/4$ of the elements of $A$ and less than $1/4$ of the elements of $A$. We can make the following observation:
\begin{enumerate}
    \item With this splitter, we will reduce the size to at most $3n/4$.
    \item Half the elements are good splitters.
\end{enumerate}

We should select splitter $s$ uniformly at random.
\begin{itemize}
    \item $P(\text{splitter is good})=\frac{1}{2}$
    \item $P(\text{splitter is bad})=\frac{1}{2}$
    \item We can show that the expected number of trials (splitter selections) until obtaining a good splitter is 2.
\end{itemize}
\subsubsection{Expected Runtime}
\begin{equation}
    \underbrace{n_0\to n_1\to n_2}_{\text{Phase 0, size}\leq n}\to \underbrace{n_3\to n_4}_{\text{Phase 1, size}\leq \frac{3}{4}n}\to \underbrace{n_5\to n_6}_{\text{Phase 2, size}\leq \frac{3}{4}^2 n}\to\dots
\end{equation}
\begin{itemize}
    \item Phase $j$: input size $\leq(\frac{3}{4})^j\,n$
    \item Random variable $y_j = $\# of recursive calls in phase $j$. Note that $E[y_j]=2$.
    \item Random variable $x_j = $\# of steps to all the recursive calls in phase $j$.
    \item Total number of steps is $x=x_0+x_1+x_2+\dots$.
    \item We can compute $E[x]=E[x_0]+E[x_1]+E[x_2]+\dots$.
    \begin{align}
        x_j&\leq c y_j \frac{3}{4}^j n\\
        E[x_j]&\leq c E[y_j] \frac{3}{4}^j n\leq 2c\frac{3}{4}^j n\\
        E[x]&=\sum_j E[x_j]\leq \sum_{j=1}^\infty 2c\frac{3}{4}^j n=\frac{2c}{1-\frac{3}{4}}n=8cn=\Theta(n)
    \end{align}
\end{itemize}

\subsubsection{Deterministic Algorithm}
\begin{itemize}
    \item If $|A|\leq 5$ then we sort $A$ and return the $k^{th}$ smallest.
    \item Otherwise, partition $A$ into $n/5$ groups of size $5$ each, then find the median of each group (constant time) and store in list $M$. This takes linear time.
    \item Select the median of $M$ with the Select algorithm, this is a good splitter.
    \item the worst case running time is $T(n)=T(\lceil\frac{n}{5}\rceil)+T(\lfloor{\frac{3n}{4}}\rfloor)+cn$.
    \item This recursive relation cannot be solved by the Master Theorem. We can prove using induction that $T(n)<20cn$.
\end{itemize}
\redbf{Question: Why groups of 5?}
\begin{itemize}
    \item With groups of 5, the total size of subproblems: $\frac{n}{5}+\frac{3n}{4}=\frac{19n}{20}<n$
    \item With groups of 3, the total size of subproblems: $\frac{n}{3}+\frac{3n}{4}=\frac{13n}{12}>n$, not sufficient.
    \item So group size of $5,7,9,11,\dots$ would also work.
\end{itemize}

\section{Closest Pair of Points}
\begin{itemize}
    \item Problem: Given a set of $n$ points, find the pair of points that are the closest in $O(n\log n)$.
\end{itemize}
\subsection{Closest Pair in 2D}
\begin{itemize}
    \item Divide: points roughly in half by drawing vertical line on midpoint
    \item Conquer: Find closest pair on each half, recursively.
    \item Combine: Find the closest pair $(p,q)$, $p\in L$, $q\in R$. However, there may be $\Theta(n^2)$ pairs.
\end{itemize}
\begin{itemize}
    \item Claim: Let $p=(x_p,y_p)\in B_L,q=(x_q,y_q)\in B_R$ with $y_p\leq y_q$. If $d(p,q)<\delta$ then there are at most \bluebf{six} other points (x,y) in $B$ such that $y_p\leq y\leq y_q$.
    \item Proof:
    \item $S_L=\{p'=(x,y):p'\neq p\in B_L\land y_p\leq y\leq y_q\}$ (other points on the left of the middle)
    \item $S_R=\{p'=(x,y):p'\neq q\in B_R\land y_p\leq y\leq y_q\}$ (other points on the right of the middle)
    \item Assume by contradiction that $|S_L\cup S_R|\geq 7$. WLOG assume $|S_L|\geq 4$.
    \item In a $\delta\times\delta$ square there are at least $4+1=5$ points. Divide the square into 4 smaller squares, by Pigeonhole Principle, there is a square with at least 2 points, whose distance is at most $\delta/\sqrt2$. This contradicts the assumption that the closest pair on the left is at most $\delta$.
\end{itemize}
\begin{itemize}
    \item Then, we can sort everything in the $y$ axis, and check the next seven points by the $y$ coordinate for the minimum distance. This takes linear time.
    \item We only need to modify the combine step in the algorithm so it's $\Theta(n)$ runtime.
\end{itemize}

\begin{algorithm}
\caption{Closest Pair in 2D}
\begin{algorithmic}[1]
\Procedure{ClosestPair}{$P$}
\State $P_x:=\text{the list of points in }P\text{ sorted by x-coordinate}$
\State $P_y:=\text{the list of points in }P\text{ sorted by y-coordinate}$
\EndProcedure
\Procedure{RCP}{$P_x,P_y$}
\If{$|P_x|\leq 3$}
\Return{$\text{brute force}(P_x)$}
\EndIf
\State $L_x:=\text{the first half of }P_x;R_x:=\text{the second half of }P_x$
\State $m:=(\text{max x-coordinate of }L_x+\text{min x-coordinate of }R_x)/2$
\State $L_y:=\text{sublist of }P_y\text{ with points in }L_x$
\State $R_y:=\text{sublist of }P_y\text{ with points in }R_x$
\State $(p_L,q_L):=\text{RCP}(L_x,L_y);(p_R,q_R):=\text{RCP}(R_x,R_y)$
\State $\delta:=\min\{d(p_L,q_L),d(p_R,q_R)\}$
\If {$\delta=d(p_L,q_L)$}
\State $p:=p_L;q:=q_L$
\Else
\State $p:=p_R;q:=q_R$
\EndIf
\State $B:=\text{sublist of }P_y\text{ with points in }[m-\delta,m+\delta]$
\Foreach {$p$ in $B$}
\Foreach {next seven $q$ after $p$ in $B$}
\If {$d(p,q)<d(p^*,q^*)$} $(p^*,q^*):=(p,q)$
\EndIf
\EndForeach
\EndForeach
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{itemize}
    \item So $T(n)=2T(\frac{n}{2})+cn$, which is $O(n\log n)$.
\end{itemize}

\section{Greedy Algorithms}
\begin{itemize}
    \item There is an optimization problem: given an input, find a solution that minimize/maximize an objective function $f$ under some constraint.
    \item Build the solution incrementally in stages
    \item At each stage, extend the solution greedily and irrevocably.
    \item For some problems this gives optimal solutions (i.e. MST), but for other problems it does not.
    \item The order of the stages is very important.
\end{itemize}

\subsection{Interval Scheduling}
\begin{itemize}
    \item Input: $n$ intervals, with interval $j$ starts at time $s_j$ and finishes at time $f_j$.
    \item Output: maximum-size set of intervals that do not overlap
    \item Naive algorithm is to try each subset of $n$ intervals by brute force $O(2^n)$, way too slow.
    \item Greedy algorithm sorts interval in some order, then if it doesn't overlap then add it to the solution.
    \item \redbf{What order gives the biggest feasible set?}
    \begin{enumerate}
        \item Increasing start time: $s_j$
        \item Increasing finish time: $e_j$
        \item Shortest interval: $e_j-s_j$
        \item Fewest conflicts
    \end{enumerate}
    \item The question is which one is optimal? Option 1 has a clear counterexample with one very long interval that overlaps each other interval. Option 3 is also not optimal using a short interval between two long interval. Option 4 is not optimal (not very clear counterexample).
    \item Option 2 is optimal. The intuition is that chosing these intervals first will leave the most time for the rest of the intervals.
    \item To find if the interval is compatible, we just need to check if the start time of the new interval is greater than the finish time of the latest scheduled interval.
\end{itemize}
\subsubsection{Proof of optimality}
\begin{itemize}
    \item Suppose for contradiction that this greedy algorithm is not optimal
    \item Say greedy selects interval $i_1,\dots,i_k$ sorted by increasing finish time.
    \item Suppose the optimal schedule $j_1,\dots,j_m$ has $m>k$ intervals, and sort by increasing finish time. Consider an optimal schedule that can match the greedy $j_1=i_1,\dots,j_r=i_r$ for the greatest possible $r$.
    \item By the nature of the greedy algorithm, then $i_{r+1}$ finishes the earliest amongst the compatible intervals remaining. So consider the schedule $S:i_1,\dots,i_r,\mathbf{i_{r+1}},j_{r+2},\dots,j_m$. 
    \item This is also optimal (contains $m$ intervals) but matches the greedy algorithm by one more position. This is a contradiction.
\end{itemize}

\subsection{Interval Partitioning}
\begin{itemize}
    \item Motivation: given a set of lecture time intervals, schedule them into as few classrooms as possible.
    \item Input: $n$ intervals, interval $j$ starts at $s_j$ and finishes at $f_j$.
    \item Output: group interval into fewest partitions such that intervals in each partition are compatible.
    \item An idea is to find the maximum set of compatible intervals using the previous algorithm. This doesn't work.
    \item We can try the same orders as the previous problem, but only earliest start time is optimal in this case.
    \item To implement it efficiently, we will use a heap with increase-key operation.
\end{itemize}
\subsubsection{Proof of optimality}
\begin{itemize}
    \item We define the \bluebf{depth} at time $t$ as the number of intervals that contain time $t$. The \bluebf{maximum depth} $d_{max}$ is the maximum depth over all times.
    \item Clearly, the number of partitions needed is at least $d_{max}$. We will show that this greedy algorithm create only $d_{max}$ partitions.
    \item Let $d$ be the number of partition the greedy algorithm opened.
    \item Partition $d$ was created because there was an interval $j$ that overlaps with some previously scheduled interval in each of the $d-1$ other partitions.
    \item This means that for $d-1$ intervals, their start times are all before $s_j$ and their finish times are all after $f_j$ (otherwise $j$ must be compatible). Hence, the depth at $s_j$ is exactly $d$.
    \item Thus, $d_{max}\geq d$ so the greedy algorithm is optimal.
    \item Warning: this proof rely on the fact that the start time
\end{itemize}

\subsection{Minimum Lateness Scheduling}
\begin{itemize}
    \item $n$ intervals: $1,2,\dots,n$, with each interval $j$ requires $t_j$ units of time and has deadline $d_j$.
    \item The lateness of an interval is $l_j=\max\{0,f_j-d_j\}$
    \item Output the schedule that minimizes the maximum lateness $L=\max_jl_j$.
    \item Fact 1: there is an optimal schedule with no gaps.
    \item The naive algorithm is to try all possible schedules, which is $O(n!)$.
    \item For greedy algorithm, we will sort intervals in this order
    \begin{enumerate}
        \item Shortest processing time, $t_j$
        \item Smallest slack first $d_j-t_j$
        \item Earliest deadline first $d_j$
    \end{enumerate}
    \item 1 and 2 are not optimal, and they have simple counterexamples with $n=2$.
    \item an \bluebf{inversion} is two intervals $i,j$ such that $d_i>d_j$ but $i$ is scheduled before $j$.
    \item \redbf{Note that in general:} Define inversion as a violation of what your order is, then prove that inversion is slightly worse or bad.
\end{itemize}
\subsubsection{Proof of optimality}
\begin{itemize}
    \item Observation 1: The greedy algorithm has no gaps.
    \item Observation 2: The greedy algorithm has no inversions.
    \item We will prove
    \item \bluebf{Claim 1}: If a schedule $S$ with no gaps has an inversion, then $S$ has a pair of inverted intervals that are adjacent.
    \item Suppose for contradiction that there is $S$ with an inversion but does not have adjacent intervals that are inverted. Then, $d_j<d_i\leq d_{i+1}\leq d_{i+2}\leq\dots\leq d_{j-1}\leq d_j$, so $d_j<d_j$ which is a contradiction.
    \item \bluebf{Claim 2}: All schedules with no gaps and no inversions have the same lateness
    \item Let $S$ and $S'$ be two distinct schedules with no gaps and no inversions
    \item Note $S$ and $S'$ differ only by the schedule of intervals with the same deadline
    \item Consider the intervals with the same deadline, they must be adjacent in both $S$ and $S'$. As a group, the maximum lateness of these intervals is the same because the group will finish at the same time.
    \item \bluebf{Claim 3}: Swapping adjacent inverted interval does not increase lateness and reduces the total number of inversions by one.
    \item Let $i$ and $j$ denote two adjacent inverted intervals in the schedule $S$. By swapping $i$ and $j$, we get a schedule $S'$ with one fewer inversion.
    \item Let $l$ and $l'$ be the lateness before/after swap. $\forall k\neq i,j, l_k=l'_k$.
    \item We know $d_i>d_j$ because it was an inversion. Thus, $l_j\geq l_j'$ and $l_j\geq l_i'$
    \item $l_j=f_j-d_j\geq f_i'-d_i=l_i'$
    \item Suppose by contradiction that the greedy schedule $S$ is not optimal.
    \item Let $S^*$ is the schedule with no gaps and fewest inversion.
    \item \textbf{Case 1:} $S^*$ has no inversion. By claim 2, this is contradiction.
    \item \textbf{Case 2:} $S^*$ has at least one inversion. By claim 1, it must have two inverted intervals that are adjacent. By claim 3, we can swap them to get a schedule with no greater lateness with one fewer inversion so it must be optimal. Then $S^*$ does not have the fewest inversions among all optimal schedules. This is also a contradiction.
\end{itemize}
\subsection{Huffman Code}
\begin{itemize}
    \item Given an alphabet $\Gamma$: a set of $n$ symbols
    \item You need to encode symbols using binary code. 
    \item Fixed length code requires $\lceil\log n\rceil$ bits per symbol.
    \item This is easy to decode, but this is not optimal
    \begin{itemize}
        \item ``e'': 12.7\% of letters in english
        \item ``z'': 0.07\% of letters in english
        \item We should give a shorter code to ``e'' than ``z''.
    \end{itemize}
    \item The goal is to find a code that minimize the length of text coding
    \item Variable length code can save space, but is harder to decode. Suppose $\Gamma=\{a,b,c\}$ and $a=1,b=01,c=010$. Then $0101$ is ambiguous as $bb$ or $ca$.
    \item A \bluebf{prefix code} is a code where no codeword is a prefix of any other. Scan from left to right until you see a codeword.
    \item Prefix code can be represented as a binary tree (edges are leaf), with the leaves as the symbols.
    \item The efficiently of a code is the weighted height of this tree, where the weight of a leaf is the probability of that symbol.
\end{itemize}
\subsection{Prefix Code Problem}
\begin{itemize}
    \item Input: A set of $\Gamma$ with their frequencies $f:\Gamma\to\R$ where $\sum_{x\in\Gamma}f(x)=1$
    \item Output: Binary tree $T$ representing the optimal prefix code for $\Gamma,f$
    \item Optimal Solution is a tree with the weighted average height $AD(T)$ of the tree is minimum.
    \item \textbf{Fact 1:} An optimal tree is a full binary tree, where each internal has two children.
    \item \textbf{Fact 2:} In an optimal tree $T$, $\forall x,y;f(x)<f(y)\implies \mathrm{depth}_T(x)\geq\mathrm{depth}_T(y)$.
    \item \textbf{Fact 3:} If $x,y$ have minimum frequency, then there is an optimal tree $T$ such that $x,y$ are siblings and are at max depth.
    \item \textbf{Huffman's Algorithm}: Combine the two smallest frequencies into a new symbol $z$ with frequency $f(z)=f(x)+f(y)$, and repeat recursively until there are only two symbol left, in which case we assign them $0$ and $1$.
\end{itemize}
\subsubsection{Proof of Optimality}
\begin{itemize}
    \item We will prove by induction. The base case for $n=2$ is trivial.
    \item Induction hypothesis: for all $\Gamma,f$ for $n-1$ symbols, the algorithm produces an optimal tree.
    \item Let $\Gamma,f$ be an alphabet with frequencies with $n$ symbols. Let $H$ be the tree produced by the algorithm.
    \item  The algorithm constructed by $H$ by: (1) replacing two symbols $x,y\in\Gamma$ of minimum frequency with a new symbol $z$, and
    \begin{align}
        \Gamma'&=(\Gamma\setminus\{x,y\})\cup\{z\}\\
        f'(\alpha)&=\begin{cases}
            f(x)+f(y),&\alpha=z\\
            f(\alpha),&\text{otherwise}
        \end{cases}
    \end{align}
    \item We know that $x,y$ are siblings (with parent $z$) in $H$
    \item Since $H'$ has $n-1$ symbols, $H'$ is optimal for $\Gamma',f'$
    \item We know $AD(H)=AD(H')+f(x)+f(y)$
    \item Now take an optimal tree $T$ for $(\Gamma,f)$ where $x,y$ are siblings. We know $T$ exists by fact 3.
    \item Let $T'$ be the tree constructed from $T$ by removing $x,y$ and replacing their parent with symbol $z$ (and $f(z)=f(x)+f(y)$)
    \item Clearly, $T'$ is a prefix code for $(\Gamma',f')$ with $AD(T)=AD(T')+f(x)+f(y)$
    \item We know $AD(T')\geq AD(H')$ as $H'$ is optimal for $(\Gamma',f')$ Thus, $AD(T)-f(x)-f(y)=AD(T')\geq AD(H')=AD(H)-f(x)-f(y)\implies AD(T)\geq AD(H)$. but $T$ is optimal so $H$ is optimal.
\end{itemize}
\subsection{Dijkstra's Single-Source Shortest Path Algorithm}
\begin{itemize}
    \item Input: A weighted directed graph $G=(V,E)$ with weights $w:E\to\R_+$ and a source vertex $s\in V$
    \item Output: Length of the shortest path from $s$ to every other node $t\in V$
    \item Suppose $R$ is a subset of nodes of $G$ that includes $s.$ A path $s\to v$ is a \bluebf{$R$-path} from $s\to v$ is restricted to contain only in $R$ before reaching $v.$
    \item At a high level, the algorithm maintains a set $R\subseteq V,$ (at the beginning $s$, and at the end $V$)
    \item The algorithm also contains $d(v)$ where
    \begin{enumerate}
        \item if $v\in R$, then $d(v)$ is the length of the shortest path from $s\to v.$ This is exactly what we want.
        \item if $v\in V\setminus R$, then $d(v)$ is the length of the shortest $s\to v$ $R$-path.
    \end{enumerate}
    \item The goal of the algorithm is to have $R$ contain all the nodes. In each iteration, the algorithm adds one more node from $R$ and maintains its properties.
    \item The algorithm will greedily choose the node $u\in V\setminus R$ with the minimum $d(u)$.
    \item We know that $d(u)$ is the length of the shortest $s\to u$ $R$-path. We will show that this is also the length of the shortest $s\to u$ path. If the claim is true, we can
    \begin{enumerate}[label=\alph*)]
        \item move $u$ to $R$ and still maintain property 1.
        \item update $d(v)$ for nodes in $V\setminus R$ to satisfy property 2.
    \end{enumerate}
    \item The proof of the claim is simple, suppose by contradiction that there is a shorter path $P$ with $len(P)<d(u).$ $P$ must cross from $R$ to $V\setminus R$ at least once. Let $v$ be the first node this $P$ reaches in $V\setminus R,$ so first section of $P,$ we denote $P_1$ from $s\to v$ is an $R$-path. As all the weights are non-negative, $len(P)\geq len(P_1)\geq d(v)\geq d(u),$ so $len(P)$ cannot be less than $d(u),$ which is a contradiction.
    \item Upon adding $u$ to $R,$ we gain more freedom for $R$-paths. For points in $V\setminus R,$ we can choose to retain the previous $R$-path or to use $u$ for the new $R$-path. Thus, for some $v\in V\setminus R,$
    \begin{itemize}
        \item if the shortest $R$-path does not contain $u,$ do nothing.
        \item if the shortest $R$-path contains $u,$ then the new $d(v)=d(u)+w(u,v)$. \begin{itemize}
            \item Note that $u$ must be the last node in the shortest $R$-path. Suppose there is another node $r\in R$ where $s\to u\to r\to v$ is shorter than $s\to u\to v.$
            \item Since $r\in R$ when $u$ was still in $V\setminus R,$ $d(r)$ must be the length of the shortest path of $R\setminus\{u\}.$ So in this case, the shortest path to $v:s\to r\to v$ doesn't require $u.$
        \end{itemize}
    \end{itemize}
    \item So for every $v\in V\setminus R,$ we update $d(v)\gets\min\{d(v),d(u)+w(u,v)\}.$
    \item \redbf{Note} that the algorithm does not work with negative weights.
\end{itemize}
\begin{algorithm}
    \caption{Dijkstra's Algorithm}
    \begin{algorithmic}[1]
    \Procedure{Dijkstra}{}
    \State $R\gets\{s\}$
    \State $d(s)\gets 0$
    for each nodes $v\neq s$ do if $(s,v)$ is an edge then $d(v)\gets w(s,v), p(v)\gets s$ else $d(v)\gets\infty,p(v)\gets Nil$
    \While{$R\neq V$}
        \State $u\gets$ node not in $R$ with the minimum $d(u)$
        \State $R\gets R\cup\{u\}$
        \Foreach{$v$ s.t. $(u,v)$ is an edge}
            \If{$d(v)>d(u)+w(u,v)$}
                \State $d(v)\gets d(u)+w(u,v)$
                \State $p(v)\gets u$
            \EndIf
        \EndForeach
    \EndWhile
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

\end{document}